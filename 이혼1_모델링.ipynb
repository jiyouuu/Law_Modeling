{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bb93e2-7b29-4ece-9c0d-9cfd200cef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "# 텍스트 데이터를 머신러닝 알고리즘이 처리할 수 있는 수치 벡터로 변환\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1b3ab4d-43ee-4107-9828-57911f919387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "390a6f40-72fc-4c7b-9518-a60a31547b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/ezen602-09/total.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58db9141-5a79-44c3-9ab7-ba233588a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "새롭게 추가된 데이터 개수: 12개\n",
      "총 데이터 개수: 4071개\n",
      "이 새로운 파일을 사용하여 모델을 다시 학습시켜 보세요!\n"
     ]
    }
   ],
   "source": [
    "# 추가할 새 데이터 \n",
    "new_data = [\n",
    "    # 폭행 / 부당한 대우\n",
    "    {'input': '남편의 폭력 때문에 더는 못 살겠어요.', 'is_divorce': 1},\n",
    "    {'input': '아내의 계속되는 폭언 때문에 정신적으로 너무 힘듭니다.', 'is_divorce': 1},\n",
    "    {'input': '배우자가 저를 무시하고 인격적으로 모독하는데, 헤어지고 싶어요.', 'is_divorce': 1},\n",
    "    {'input': '지속적인 가정폭력으로 신변에 위협을 느껴 이혼을 결심했습니다.', 'is_divorce': 1},\n",
    "    \n",
    "    # 외도 / 부정행위\n",
    "    {'input': '배우자가 바람피운 사실을 알게 됐습니다. 관계를 끝내고 싶어요.', 'is_divorce': 1},\n",
    "    {'input': '상간녀 소송을 준비하면서 이혼도 같이 진행하려고요.', 'is_divorce': 1},\n",
    "    \n",
    "    # 경제적 문제\n",
    "    {'input': '남편이 생활비를 전혀 주지 않아 이혼을 생각 중입니다.', 'is_divorce': 1},\n",
    "    {'input': '아내가 저 몰래 큰 빚을 졌는데, 감당이 안 됩니다.', 'is_divorce': 1},\n",
    "    {'input': '배우자가 도박에 빠져서 가산을 탕진했어요.', 'is_divorce': 1},\n",
    "    \n",
    "    # 기타 사유\n",
    "    {'input': '성격 차이가 너무 심해서 헤어지기로 마음먹었어요.', 'is_divorce': 1},\n",
    "    {'input': '시댁과의 갈등으로 혼인 관계를 유지하기 힘듭니다.', 'is_divorce': 1},\n",
    "    {'input': '별거한 지 오래되었고, 이제 서류 정리를 하고 싶어요.', 'is_divorce': 1},\n",
    "]\n",
    "df_new = pd.DataFrame(new_data)\n",
    "df_updated = pd.concat([df, df_new], ignore_index=True)\n",
    "df_updated.to_csv(\"C:/Users/ezen602-09/total.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n새롭게 추가된 데이터 개수: {len(df_new)}개\")\n",
    "print(f\"총 데이터 개수: {len(df_updated)}개\")\n",
    "print(\"이 새로운 파일을 사용하여 모델을 다시 학습시켜 보세요!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2ceef-314f-4ddb-b23b-7cdd722e5e3f",
   "metadata": {},
   "source": [
    "# KoNLPy를 사용한 정교한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "000ed6d2-4c03-47ad-b845-6f4c20b14bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 전처리 전/후 비교 ===\n",
      "원본 [0]: 상고에서 상고이유서를 제출하는 기간의 법적 효력은 어떻게 됩니까?\n",
      "결과 [0]: 상고 유서 제출 기간 법적\n",
      "\n",
      "원본 [1]: 정신적 고통에 대한 손해배상은 어떤 경우에 인정되나요?\n",
      "결과 [1]: 정신 고통 손해배상\n",
      "\n",
      "원본 [2]: 양도소득세 납부와 관련한 채무불이행이 인정되는 경우, 피해자는 몇 가지 손해를 청구할 수 있나요?\n",
      "결과 [2]: 양도소득세 납부 채무불이행 피해자 손해 청구\n",
      "\n",
      "원본 [3]: 퇴직금 지급을 위한 근로자의 사직 의사표시가 유효하기 위한 조건은 무엇인가요?\n",
      "결과 [3]: 퇴직금 지급 근로자 사직 의사표시\n",
      "\n",
      "원본 [4]: 퇴직일이 실제 근로 종료일과 다른 경우 퇴직금 청구에 어떤 영향을 미치나요?\n",
      "결과 [4]: 퇴직 일이 실제 근 종료 다른 퇴직금 청구 영향 미치나\n",
      "\n",
      "=== 각 목적에 맞게 생성된 전처리 컬럼들 ===\n",
      "                                               input  \\\n",
      "0               상고에서 상고이유서를 제출하는 기간의 법적 효력은 어떻게 됩니까?   \n",
      "1                     정신적 고통에 대한 손해배상은 어떤 경우에 인정되나요?   \n",
      "2  양도소득세 납부와 관련한 채무불이행이 인정되는 경우, 피해자는 몇 가지 손해를 청구...   \n",
      "3        퇴직금 지급을 위한 근로자의 사직 의사표시가 유효하기 위한 조건은 무엇인가요?   \n",
      "4         퇴직일이 실제 근로 종료일과 다른 경우 퇴직금 청구에 어떤 영향을 미치나요?   \n",
      "\n",
      "                  input_processed  \n",
      "0                  상고 유서 제출 기간 법적  \n",
      "1                      정신 고통 손해배상  \n",
      "2        양도소득세 납부 채무불이행 피해자 손해 청구  \n",
      "3              퇴직금 지급 근로자 사직 의사표시  \n",
      "4  퇴직 일이 실제 근 종료 다른 퇴직금 청구 영향 미치나  \n"
     ]
    }
   ],
   "source": [
    "MINIMAL_STOPWORDS = [\n",
    "    '것', '수', '때', '등', '들', '더', '이', '그', '저', '나', \n",
    "    '우리', '같', '또', '만', '년', '월', '일', '하다', '있다', '되다', '가하다','이루어지다',\n",
    "     '가능하다', '가능', '가다', '되다', '하다', '있다', '없다', '않다', '된다', '한다',\n",
    "    '어떻게', '어떤', '무엇', '언제', '어디서', '왜', '누가', '얼마', '몇',\n",
    "    '알고', '싶다', '궁금하다', '문의', '질문', '답변', '설명', '이해',\n",
    "    '과정', '절차', '이후', '다음', '먼저', '그리고', '그러나', '하지만', '그래서', '제자','제호','제조',\n",
    "\n",
    "     '없', '있', '하', '되', '않', \n",
    "    '나', '우리', '너', '당신', '같', '또', '것', '때', '등', \n",
    "    '때문', '정도', '사실', '생각', '경우', '문제', '방법', '상황', '내용', '결과', '사람',\n",
    "\n",
    "    '해야', '하면', '경우', '때는', '어느', '무슨', '어디', '누구' , ' 가지다' , '가지', '하나요', '위해', '이혼',\n",
    "     '조건', '대한', '관련', '따르다', '판단', '인정', '적용', \n",
    "    '효력', '성립', '발생', '요건', '증명', '근거', '주장'\n",
    "]\n",
    "\n",
    "\n",
    "# # 법률 전문용어는 무조건 포함\n",
    "LEGAL_KEYWORDS = [ '위자료', '재산분할', '양육권', '친권', '면접교섭', \n",
    "    '협의이혼', '청구', '배상', '손해', '책임',\n",
    "    '차용금', '반환', '취소', '원상회복', '사해행위', '채권자', '배우자', '이혼사유',\n",
    "     '이혼', '사유', \n",
    "    '혼인', '금전거래', '청구권',  '액수', '정해지', '부적법',\n",
    "    '혼인파탄', '파탄', '분할', '양육비', '면접',\n",
    "    '교섭', '협의', '조정신청', '손해배상', '부부', '배우자', '당사자', '사람', '개인', '상대방', \n",
    "]\n",
    "\n",
    "\n",
    "def preprocess_text(text):  # 전처리 함수 \n",
    "    \"\"\"\n",
    "    한국어 텍스트를 입력받아 전처리하는 함수:\n",
    "    1. '제3자' 형태의 단어를 임시 토큰으로 보호/복원하여 숫자를 보존.\n",
    "    2. 불필요한 한글 이외의 문자를 제거.\n",
    "    3. 형태소 분석 및 어간 추출.\n",
    "    4. 불용어 및 1글자 단어 제거.\n",
    "    \"\"\"\n",
    "    # 1단계: 한글, 공백을 제외한 모든 문자 제거\n",
    "    # re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)는 한글과 공백만 남기고 나머지는 지우라는 의미\n",
    "    # 제숫자 + 한글 글자 + 선택적 조사까지 포함\n",
    "    # 딕셔너리를 함수 내부에 선언하여 매 호출마다 초기화 (핵심 수정 사항)\n",
    "    protected_matches = {}\n",
    "\n",
    "    def protect_term(match):\n",
    "        # 찾은 문자열(예: '제3자')을 고유한 토큰(예: '###TOKEN0###')으로 매핑\n",
    "        token = f\"###TOKEN{len(protected_matches)}###\"\n",
    "        protected_matches[token] = match.group(0)\n",
    "        return match.group(0) # 일단 원본 단어 그대로 둔 채로 형태소 분석 진행\n",
    "    def strip_josa(text):\n",
    "    # 한글 명사 뒤 “의”, “가”, “을” 등 제거\n",
    "        return re.sub(r'([가-힣]+)(의|가|를|은|는|과|와|에|로|으로)$', r'\\1', text)\n",
    "\n",
    "    # 1단계: '제3자' 형태의 단어를 임시 토큰으로 치환하여 보호\n",
    "    text = re.sub(r'(제\\d+[가-힣]+)', protect_term, text)\n",
    "    # 2단계: 한글, 공백, 보호 토큰의 문자(#)만 남기고 제거\n",
    "    text = re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣# ]', '', text)\n",
    "\n",
    "    # 2단계: Okt 형태소 분석기를 이용한 토큰화 및 어간 추출(Stemming)\n",
    "    # okt.pos(text, stem=True)는 문장을 (단어, 품사) 형태로 나누고, '하다', '먹다'처럼 원형으로 만들어줍니다.\n",
    "    # 예: \"먹었었고\" -> ('먹다', 'Verb')\n",
    "    # Okt 형태소 분석기 객체 생성\n",
    "    okt = Okt() \n",
    "    word_tokens = okt.pos(text, stem=True)\n",
    "\n",
    "    # 3단계: 불용어 제거\n",
    "    # 형태소 분석 후 의미 있는 단어만 추출하는 필터링 과정\n",
    "    # 의미를 가지는 명사, 동사, 형용사, 부사 중에서 1글자 이상인 단어만 추출합니다.\n",
    "    # 품사 태그가 'Josa'(조사), 'Eomi'(어미), 'Punctuation'(구두점) 등인 단어들을 제거합니다.\n",
    "    meaningful_words = []\n",
    "    for word, pos in word_tokens:\n",
    "        if word in LEGAL_KEYWORDS:\n",
    "            meaningful_words.append(word)\n",
    "        elif pos in ['Noun','Verb'] and len(word) > 1:\n",
    "            token = strip_josa(word)\n",
    "            if token not in MINIMAL_STOPWORDS:\n",
    "                meaningful_words.append(token)\n",
    "     # 6. 보호된 단어 강제 포함 및 복원 (핵심)\n",
    "    # 보호 목록에 있던 단어들을 강제로 최종 리스트에 추가합니다.\n",
    "    # (이미 리스트에 분해되어 들어갔을 수 있지만, 완벽한 복원을 위해 강제 추가)\n",
    "    for original_term in protected_matches.values():\n",
    "        # '제3자' 자체를 하나의 단어로 명시적으로 추가\n",
    "        meaningful_words.append(original_term)\n",
    "        \n",
    "    # 7. 중복 제거 및 최종 문자열 반환\n",
    "    final_words = list(dict.fromkeys(meaningful_words))\n",
    "    # 최종적으로 공백으로 연결된 문자열을 반환합니다. \n",
    "    # 모델에 따라 리스트 형태(meaningful_words)를 그대로 사용할 수도 있습니다.\n",
    "    # 텍스트에서 분석에 의미 있는 핵심 단어들만 남긴 리스트 생성\n",
    "    return ' '.join(final_words)\n",
    "\n",
    "# 각 목적에 맞게 전처리 컬럼 생성 ---\n",
    "# 질문 의도 파악 모델용: 'input' 컬럼만 전처리\n",
    "# 'input' 컬럼에 전처리 함수를 적용하여 새로운 'input_processed' 컬럼 생성\n",
    "df['input_processed'] = df['input'].apply(preprocess_text)\n",
    "\n",
    "# 결과 비교를 위해 원본과 처리된 결과를 나란히 출력\n",
    "print(\"\\n=== 전처리 전/후 비교 ===\")\n",
    "for i in range(5):\n",
    "    print(f\"원본 [{i}]: {df['input'].iloc[i]}\")\n",
    "    print(f\"결과 [{i}]: {df['input_processed'].iloc[i]}\\n\")\n",
    "\n",
    "\n",
    "# 전처리된 데이터가 포함된 데이터프레임 확인\n",
    "print(\"=== 각 목적에 맞게 생성된 전처리 컬럼들 ===\")\n",
    "print(df[['input', 'input_processed']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ea94566-9cb1-4e75-88b3-75c3be27160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # 랜덤 셔플링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa688057-7704-4dd6-b483-98d3c78bfdb5",
   "metadata": {},
   "source": [
    "#  학습/테스트 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7650805b-5048-4184-8de5-2539c47f289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "학습 데이터: 3247개\n",
      "테스트 데이터: 812개\n"
     ]
    }
   ],
   "source": [
    "# 3. 학습/테스트 데이터 분할\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    print(f\"\\n학습 데이터: {len(X_train)}개\")\n",
    "    print(f\"테스트 데이터: {len(X_test)}개\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ec94d-a1f7-4bc7-ac17-a01715d2c599",
   "metadata": {},
   "source": [
    "# 토크나이저 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af3d96-3b9a-431c-ada3-b5b1b37f2b05",
   "metadata": {},
   "source": [
    "# 커스텀 데이터셋 정의\n",
    "DivorceDataset 클래스는 PyTorch의 기본 Dataset 클래스를 상속받아 텍스트 데이터와 레이블(정답)을 모델이 학습할 수 있는 형태로 변환하고 제공하는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42b86c4-987d-476f-9dbb-e32f19a2e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivorceDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts  #입력 텍스트 데이터 목록 (예: 문장 리스트)\n",
    "        self.labels = labels #각 텍스트에 해당하는 정답 레이블 목록 (예: 0 또는 1)\n",
    "        self.tokenizer = tokenizer  # 텍스트를 모델이 이해하는 숫자로 변환하는 데 사용되는 토크나이저 객체\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):  # DataLoader가 데이터셋의 크기를 파악하는 데 사용\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        enc = self.tokenizer(  # 저장된 토크나이저를 사용하여 텍스트를 전처리\n",
    "            text,\n",
    "            max_length=self.max_len,  # 텍스트 길이를 max_len으로 제\n",
    "            truncation=True,  # max_len을 초과하는 텍스트는 잘라냄\n",
    "            padding='max_length', # max_len보다 짧은 텍스트는 부족한 길이를 0으로 채워 길이를 맞춤\n",
    "            return_tensors='pt'  # 결과를 PyTorch 텐서(tensor) 형태로 반환하도록 지정\n",
    "        )\n",
    "        return {  # 모델 학습에 필요한 다음 세 가지 요소가 딕셔너리 형태로 반환\n",
    "            'input_ids': enc['input_ids'].squeeze(),  # 텍스트의 각 단어/토큰이 매핑된 고유한 정수 ID 시퀀스\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),  # 시퀀스에서 실제 데이터(토큰)와 패딩(채워 넣은 0)을 구분하기 위한 마스크. (실제 토큰은 1, 패딩 토큰은 0)\n",
    "            'labels': torch.tensor(label, dtype=torch.long) # 해당 텍스트의 정답 레이블 (PyTorch 롱 텐서 형태로 변환)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ceac76-3e23-48d8-aa7f-575a555c4be8",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader 생성\n",
    "머신러닝 모델 학습을 효율적이고 정확하게 수행하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9618e287-bb90-4c43-a300-3107c600c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PyTorch와 BERT/KoBERT 모델이 요구하는 표준화된 형식으로 변환하고 캡슐화\n",
    "train_dataset = DivorceDataset(X_train,y_train, tokenizer)\n",
    "test_dataset = DivorceDataset(X_test,  y_test, tokenizer)\n",
    "\n",
    "# DataLoader는 생성된 Dataset 객체로부터 데이터를 불러와 모델 학습에 적합한 형태로 관리하고 공급하는 핵심 도구\n",
    "# batch_size=16 메모리 제한: 전체 데이터를 한 번에 메모리에 올리는 것을 방지하고, 학습 안정성을 높임\n",
    "# shuffle=True 편향 방지: 데이터의 특정 순서(예: 긍정 데이터가 먼저 나오고 부정 데이터가 나중에 나오는 경우)에 모델이 익숙해지는 것을 방지하여 일반화 성능을 높임\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9699afc-9c1c-4e7d-9f58-f124620d0f83",
   "metadata": {},
   "source": [
    "# KoBERT 모델 로드\n",
    "BertModel : 입력 텍스트를 이해하고 특징을 추출하는 코어 언어 모델\n",
    "encoder :   BERT 모델의 핵심 부분으로, 텍스트의 문맥적 특징을 추출\n",
    "pooler : 인코더의 최종 출력을 분류 작업에 사용할 수 있는 고정된 크기의 벡터로 압축\n",
    "\n",
    "분류를 위한 헤드: (classifier) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba7337-c4d6-4594-8ed8-f5e4e27d0d61",
   "metadata": {},
   "source": [
    "# PyTorch에서 BERT 모델을 fine-tuning(학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb6ec1-3b9c-4913-b8ac-b58825e4cff0",
   "metadata": {},
   "source": [
    "# Optimizer 설정\n",
    "모델이 데이터에서 학습할 수 있도록 방향을 잡아주는 역할\n",
    "lr=2e-5 → 학습률, 너무 크면 발산, 너무 작으면 느리게 수렴\n",
    "AdamW: 모델을 훈련시키는 데 사용되는 최적화 알고리즘 => Transformer 모델에서 흔히 발생하는 가중치 감소(Weight Decay) 문제를 더 잘 처리하도록 설계됨 \n",
    "AdamW 옵티마이저가 2e−5의 속도로 KoBERT 모델의 모든 가중치를 수정하여 최적의 성능을 찾도록 준비하는 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5851f33-b4b1-4785-9eab-31d662eef3e6",
   "metadata": {},
   "source": [
    "# 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2426499d-5cd2-4447-952b-da1297ac71bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cpu\n",
      "GPU 사용 가능: False\n",
      "==================================================\n",
      "KoBERT 이혼 vs 비이혼 분류 모델 학습 시작\n",
      "==================================================\n",
      "\n",
      "=== 데이터 준비 ===\n",
      "전체 데이터: 4071개\n",
      "이혼: 721개 (17.7%)\n",
      "비이혼: 3350개 (82.3%)\n",
      "\n",
      "토크나이저 로드 중...\n",
      "\n",
      "학습 데이터: 2930개\n",
      "검증 데이터: 326개\n",
      "테스트 데이터: 815개\n",
      "\n",
      "모델 초기화 중...\n",
      "\n",
      "=== Epoch 1/3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350ac42cc39b416da4acb2dcd6b8b998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2626 | Train Acc: 0.9096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3e4827cdf141c582a39cb27d5ea403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0889 | Val Acc: 0.9816\n",
      "✅ 모델 저장! (Val Acc: 0.9816)\n",
      "\n",
      "=== Epoch 2/3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287dcbdeb22847edae1419c336e8555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 438\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# 10. 사용 예시\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m--> 438\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_kobert_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# 새로운 텍스트 예측\u001b[39;00m\n\u001b[0;32m    441\u001b[0m test_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m남편이 날 때려서 헤어지고 싶어\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이혼 소송을 진행하고 싶습니다\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m계약서 작성 문의드립니다\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[29], line 412\u001b[0m, in \u001b[0;36mrun_kobert_pipeline\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    405\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_linear_schedule_with_warmup(\n\u001b[0;32m    406\u001b[0m     optimizer,\n\u001b[0;32m    407\u001b[0m     num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(total_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m    408\u001b[0m     num_training_steps\u001b[38;5;241m=\u001b[39mtotal_steps\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# 6. 모델 학습\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# 7. 학습 히스토리 시각화\u001b[39;00m\n\u001b[0;32m    418\u001b[0m plot_training_history(history)\n",
      "Cell \u001b[1;32mIn[29], line 254\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, scheduler, device, epochs)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# 검증\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 191\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, optimizer, scheduler, device)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    190\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 191\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Gradient clipping (폭발 방지)\u001b[39;00m\n\u001b[0;32m    194\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_new_env\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_new_env\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my_new_env\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "KoBERT 기반 이혼 vs 비이혼 분류 모델\n",
    "- 문맥 이해 능력 우수\n",
    "- 사전학습된 한국어 BERT 모델 활용\n",
    "- Fine-tuning으로 도메인 적응\n",
    "\n",
    "필요한 라이브러리 설치:\n",
    "pip install torch transformers\n",
    "pip install kobert-transformers\n",
    "pip install scikit-learn pandas numpy matplotlib seaborn\n",
    "\"\"\"\n",
    "\n",
    "# ========================================\n",
    "# 1. 설정 및 하이퍼파라미터\n",
    "# ========================================\n",
    "class Config:\n",
    "    # 모델 설정\n",
    "    MODEL_NAME = 'monologg/kobert'  # KoBERT 사전학습 모델\n",
    "    MAX_LEN = 128  # 최대 토큰 길이\n",
    "    BATCH_SIZE = 16  # 배치 크기 (GPU 메모리에 따라 조정)\n",
    "    EPOCHS = 3  # 학습 에폭 (3-5 추천)\n",
    "    LEARNING_RATE = 2e-5  # 학습률\n",
    "    DROPOUT_RATE = 0.3  # 드롭아웃 비율\n",
    "    \n",
    "    # 데이터 설정\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.1  # Validation set 비율\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # 기타\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    SAVE_PATH = 'kobert_divorce_model.pt'\n",
    "\n",
    "config = Config()\n",
    "print(f\"사용 디바이스: {config.DEVICE}\")\n",
    "print(f\"GPU 사용 가능: {torch.cuda.is_available()}\")\n",
    "\n",
    "# ========================================\n",
    "# 2. 데이터셋 클래스\n",
    "# ========================================\n",
    "class DivorceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    이혼/비이혼 분류를 위한 커스텀 데이터셋\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 토크나이저로 인코딩\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # [CLS], [SEP] 추가\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ========================================\n",
    "# 3. KoBERT 분류 모델\n",
    "# ========================================\n",
    "class KoBERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    KoBERT + 분류 헤드\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=2, dropout=0.3):\n",
    "        super(KoBERTClassifier, self).__init__()\n",
    "        \n",
    "        # KoBERT 모델 로드\n",
    "        self.bert = BertModel.from_pretrained('monologg/kobert', trust_remote_code=True)\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERT 인코딩\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # [CLS] 토큰의 hidden state 사용\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # 드롭아웃 + 분류\n",
    "        output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ========================================\n",
    "# 4. 데이터 준비 함수\n",
    "# ========================================\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    데이터 로드 및 전처리\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 데이터 준비 ===\")\n",
    "    \n",
    "    # 텍스트와 레이블 추출\n",
    "    texts = df['input_processed'].values  # 질의 전처리 데이터\n",
    "    labels = df['is_divorce'].values # 0 또는 1\n",
    "\n",
    "    \n",
    "    # 데이터 분포 확인\n",
    "    print(f\"전체 데이터: {len(df)}개\")\n",
    "    print(f\"이혼: {sum(labels == 1)}개 ({sum(labels == 1)/len(labels)*100:.1f}%)\")\n",
    "    print(f\"비이혼: {sum(labels == 0)}개 ({sum(labels == 0)/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "def create_data_loaders(texts, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Train/Val/Test 데이터로더 생성\n",
    "    \"\"\"\n",
    "    # Train/Test 분할\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        texts, labels,\n",
    "        test_size=config.TEST_SIZE,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Train/Val 분할\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        train_texts, train_labels,\n",
    "        test_size=config.VAL_SIZE,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n학습 데이터: {len(train_texts)}개\")\n",
    "    print(f\"검증 데이터: {len(val_texts)}개\")\n",
    "    print(f\"테스트 데이터: {len(test_texts)}개\")\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = DivorceDataset(train_texts, train_labels, tokenizer, config.MAX_LEN)\n",
    "    val_dataset = DivorceDataset(val_texts, val_labels, tokenizer, config.MAX_LEN)\n",
    "    test_dataset = DivorceDataset(test_texts, test_labels, tokenizer, config.MAX_LEN)\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ========================================\n",
    "# 5. 학습 함수\n",
    "# ========================================\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    1 에폭 학습\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Loss 계산 (CrossEntropyLoss에 class_weight 적용 가능)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (폭발 방지)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 통계\n",
    "        losses.append(loss.item())\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    return correct_predictions.double() / total_samples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    모델 평가\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct_predictions.double() / total_samples\n",
    "    return accuracy, np.mean(losses), all_preds, all_labels\n",
    "\n",
    "# ========================================\n",
    "# 6. 전체 학습 파이프라인\n",
    "# ========================================\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, epochs):\n",
    "    \"\"\"\n",
    "    전체 학습 루프\n",
    "    \"\"\"\n",
    "    best_accuracy = 0\n",
    "    history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\n=== Epoch {epoch + 1}/{epochs} ===')\n",
    "        \n",
    "        # 학습\n",
    "        train_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "        \n",
    "        # 검증\n",
    "        val_acc, val_loss, _, _ = eval_model(model, val_loader, device)\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # 히스토리 저장\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc.item())\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # 최고 모델 저장\n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), config.SAVE_PATH)\n",
    "            best_accuracy = val_acc\n",
    "            print(f'✅ 모델 저장! (Val Acc: {val_acc:.4f})')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# ========================================\n",
    "# 7. 평가 및 시각화\n",
    "# ========================================\n",
    "def evaluate_and_visualize(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    테스트 데이터 평가 및 결과 시각화\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 테스트 데이터 평가 ===\")\n",
    "    \n",
    "    # 최고 모델 로드\n",
    "    model.load_state_dict(torch.load(config.SAVE_PATH))\n",
    "    \n",
    "    test_acc, test_loss, y_pred, y_true = eval_model(model, test_loader, device)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "    print(f'F1 Score: {f1_score(y_true, y_pred):.4f}')\n",
    "    \n",
    "    # 분류 리포트\n",
    "    print(\"\\n분류 리포트:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['비이혼', '이혼'], digits=4))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['비이혼', '이혼'],\n",
    "                yticklabels=['비이혼', '이혼'])\n",
    "    plt.title('Confusion Matrix - KoBERT')\n",
    "    plt.ylabel('실제')\n",
    "    plt.xlabel('예측')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kobert_confusion_matrix.png', dpi=300)\n",
    "    print(\"\\n✅ Confusion Matrix 저장: kobert_confusion_matrix.png\")\n",
    "    \n",
    "    return y_pred, y_true\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    학습 히스토리 시각화\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history['train_acc'], label='Train Accuracy')\n",
    "    ax1.plot(history['val_acc'], label='Val Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(history['train_loss'], label='Train Loss')\n",
    "    ax2.plot(history['val_loss'], label='Val Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kobert_training_history.png', dpi=300)\n",
    "    print(\"✅ 학습 히스토리 저장: kobert_training_history.png\")\n",
    "\n",
    "# ========================================\n",
    "# 8. 예측 함수\n",
    "# ========================================\n",
    "def predict_text(model, tokenizer, text, device):\n",
    "    \"\"\"\n",
    "    새로운 텍스트 예측\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config.MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1)\n",
    "    \n",
    "    result = \"이혼\" if pred.item() == 1 else \"비이혼\"\n",
    "    confidence = probs[0][pred.item()].item() * 100\n",
    "    \n",
    "    print(f\"\\n예측 결과: {result}\")\n",
    "    print(f\"신뢰도: {confidence:.2f}%\")\n",
    "    print(f\"확률 - 비이혼: {probs[0][0].item():.4f}, 이혼: {probs[0][1].item():.4f}\")\n",
    "    \n",
    "    return pred.item(), probs[0].cpu().numpy()\n",
    "\n",
    "# ========================================\n",
    "# 9. 메인 실행 함수\n",
    "# ========================================\n",
    "def run_kobert_pipeline(df):\n",
    "    \"\"\"\n",
    "    KoBERT 전체 파이프라인 실행\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"KoBERT 이혼 vs 비이혼 분류 모델 학습 시작\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. 데이터 준비\n",
    "    texts, labels = prepare_data(df)\n",
    "    \n",
    "    # 2. 토크나이저 로드\n",
    "    print(\"\\n토크나이저 로드 중...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('monologg/kobert',  trust_remote_code=True)\n",
    "   \n",
    "    # 3. 데이터로더 생성\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(texts, labels, tokenizer)\n",
    "    \n",
    "    # 4. 모델 초기화\n",
    "    print(\"\\n모델 초기화 중...\")\n",
    "    model = KoBERTClassifier(n_classes=2, dropout=config.DROPOUT_RATE)\n",
    "    model = model.to(config.DEVICE)\n",
    "    \n",
    "    # 5. Optimizer & Scheduler 설정\n",
    "    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    total_steps = len(train_loader) * config.EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(total_steps * 0.1),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 6. 모델 학습\n",
    "    history = train_model(\n",
    "        model, train_loader, val_loader, \n",
    "        optimizer, scheduler, config.DEVICE, config.EPOCHS\n",
    "    )\n",
    "    \n",
    "    # 7. 학습 히스토리 시각화\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # 8. 테스트 평가\n",
    "    y_pred, y_true = evaluate_and_visualize(model, test_loader, config.DEVICE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"학습 완료!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ========================================\n",
    "# 10. 사용 예시\n",
    "# ========================================\n",
    "\n",
    "# 데이터 로드\n",
    "# df = pd.read_csv('divorce_data.csv')\n",
    "# # 필요 컬럼: 'text', 'is_divorce'\n",
    "\n",
    "# 모델 학습\n",
    "model, tokenizer = run_kobert_pipeline(df)\n",
    "\n",
    "# 새로운 텍스트 예측\n",
    "test_texts = [\n",
    "    \"남편이 날 때려서 헤어지고 싶어\",\n",
    "    \"이혼 소송을 진행하고 싶습니다\",\n",
    "    \"계약서 작성 문의드립니다\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\n입력: {text}\")\n",
    "    predict_text(model, tokenizer, text, config.DEVICE)\n",
    "\n",
    "# 모델 저장 (이미 학습 중 저장됨)\n",
    "# torch.save(model.state_dict(), 'kobert_divorce_final.pt')\n",
    "\n",
    "# 모델 로드\n",
    "# model.load_state_dict(torch.load('kobert_divorce_final.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0295fcb7-088b-48ff-9833-667ae7d37172",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"kobert_divorce_parttt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767c7c6-a926-48a0-8c03-a445a8ddfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"kobert_divorce_final.pt\")\n",
    "# 모델 로드\n",
    "model.load_state_dict(torch.load(\"kobert_divorce_final.pt\"))ㅇ\n",
    "model.to(device)\n",
    "\n",
    "# optimizer도 그대로 이어서 설정 가능\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 다시 학습 루프 실행"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "my_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
