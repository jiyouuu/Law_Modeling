{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e55b1e1-3333-4767-b8dd-740574ebd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a4a5c1-8c57-4d65-9350-0862669910ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Index(['score', 'confidence', 'classification', 'tier1_count', 'tier2_count',\n",
      "       'tier3_count', 'keyword_density', 'text_length', 'reason', 'doc_id',\n",
      "       'casetype', 'casenames', 'input', 'output', 'title', 'fileName',\n",
      "       'announce_date', 'decision_date', 'response_date', 'row_index',\n",
      "       'is_divorce'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_not = pd.read_csv(\"C:/Users/82105/Downloads/divorce_not_related_true.csv\", encoding=\"utf-8-sig\") # ì´í˜¼ ì•„ë‹˜\n",
    "df = pd.read_csv(\"C:/Users/82105/Downloads/merged_score.csv\", encoding=\"utf-8-sig\")   #ì´í˜¼ \n",
    "\n",
    "df['is_divorce'] = 1\n",
    "df_not['is_divorce'] = 0\n",
    "\n",
    "# ë‘ ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°\n",
    "combined_df = pd.concat([df, df_not], ignore_index=True)\n",
    "\n",
    "cols = combined_df.columns\n",
    "print(len(cols))   # ì»¬ëŸ¼ ê°œìˆ˜ í™•ì¸\n",
    "print(cols)  \n",
    "\n",
    "combined_df.to_csv(\"C:/Users/82105/Downloads/combined2.csv\", index=False, encoding='utf-8-sig')\n",
    "combined_df = pd.read_csv(\"C:/Users/82105/Downloads/combined.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ë¥¼ ì ìš©í•  ì»¬ëŸ¼ì„ í™•ì¸í•©ë‹ˆë‹¤. 'input'ê³¼ 'output' ì»¬ëŸ¼ì´ ì¤‘ìš”í•´ ë³´ì…ë‹ˆë‹¤.\n",
    "# í˜¹ì‹œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°(NaN)ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›Œì¤ë‹ˆë‹¤.\n",
    "combined_df['input'] = combined_df['input'].fillna('')\n",
    "combined_df['output'] = combined_df['output'].fillna('')\n",
    "\n",
    "# ëª¨ë¸ 3 : ë‹µë³€ ê²€ìƒ‰ ëª¨ë¸ì„ ìœ„í•œ 'í†µí•©' í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìƒì„±\n",
    "combined_df['text_combined'] = combined_df['input'].astype(str) + \" \" + combined_df['output'].astype(str)\n",
    "\n",
    "print(\"ì „ì²˜ë¦¬ ì „ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "print(combined_df[['input', 'output']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "64876aa3-6673-4290-9bf7-04c07798bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv(\"C:/Users/82105/Downloads/combined_before_jeon.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56272af6-c515-496a-80e9-43ed43d497f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2.to_csv(\"C:/Users/82105/Downloads/combined_before_jeon.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6d196ec-db38-4418-839c-f6e403f63fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì´í˜¼ ê´€ë ¨ ë¬¸ì„œë§Œ í† í”½ ëª¨ë¸ë§ ì‹œì‘ ===\n",
      "ğŸ“Š ë°ì´í„° ë¶„í• :\n",
      "  â€¢ ì´í˜¼ ê´€ë ¨ ë¬¸ì„œ: 741ê°œ\n",
      "  â€¢ ë¹„ì´í˜¼ ê´€ë ¨ ë¬¸ì„œ: 3706ê°œ\n",
      "  â€¢ ì „ì²´: 4447ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ì´í˜¼ ê´€ë ¨ ë¬¸ì„œë§Œ í† í”½ ëª¨ë¸ë§ ì‹œì‘ ===\")\n",
    "divorce_df = combined_df[combined_df['is_divorce'] == 1].copy()\n",
    "non_divorce_df = combined_df[combined_df['is_divorce'] == 0].copy()\n",
    "\n",
    "print(f\"ğŸ“Š ë°ì´í„° ë¶„í• :\")\n",
    "print(f\"  â€¢ ì´í˜¼ ê´€ë ¨ ë¬¸ì„œ: {len(divorce_df)}ê°œ\")\n",
    "print(f\"  â€¢ ë¹„ì´í˜¼ ê´€ë ¨ ë¬¸ì„œ: {len(non_divorce_df)}ê°œ\")\n",
    "print(f\"  â€¢ ì „ì²´: {len(combined_df)}ê°œ\")\n",
    "\n",
    "# ì´í˜¼ ë¬¸ì„œë§Œìœ¼ë¡œ í† í”½ ëª¨ë¸ë§\n",
    "divorce_df['topic_label'] = -1  # ë¹ˆ ì»¬ëŸ¼ ìƒì„±\n",
    "\n",
    " # ë¹„ì´í˜¼ ë¬¸ì„œëŠ” topic_label = -1 ìœ ì§€\n",
    "non_divorce_df['topic_label'] = -1  # ë¯¸ë¶„ë¥˜ë¡œ ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "68d7e304-b176-49e7-8322-93b5a237d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "divorce_df.to_csv(\"C:/Users/82105/Downloads/divorce_df.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b90b4-f637-4e36-85e5-403904366883",
   "metadata": {},
   "source": [
    "# 1ë‹¨ê³„ : KoNLPyë¥¼ ì‚¬ìš©í•œ ì •êµí•œ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2827432b-b903-46b0-bbbb-4c6d4526317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAL_STOPWORDS = [\n",
    "    'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë“±', 'ë“¤', 'ë”', 'ì´', 'ê·¸', 'ì €', 'ë‚˜', \n",
    "    'ìš°ë¦¬', 'ê°™', 'ë˜', 'ë§Œ', 'ë…„', 'ì›”', 'ì¼', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤',\n",
    "\n",
    "     'ê°€ëŠ¥í•˜ë‹¤', 'ê°€ëŠ¥', 'ê°€ë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì•Šë‹¤', 'ëœë‹¤', 'í•œë‹¤',\n",
    "    'ì–´ë–»ê²Œ', 'ì–´ë–¤', 'ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ì™œ', 'ëˆ„ê°€', 'ì–¼ë§ˆ', 'ëª‡',\n",
    "    'ì•Œê³ ', 'ì‹¶ë‹¤', 'ê¶ê¸ˆí•˜ë‹¤', 'ë¬¸ì˜', 'ì§ˆë¬¸', 'ë‹µë³€', 'ì„¤ëª…', 'ì´í•´',\n",
    "    'ê³¼ì •', 'ì ˆì°¨', 'ì´í›„', 'ë‹¤ìŒ', 'ë¨¼ì €', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ì œì','ì œí˜¸',\n",
    "\n",
    "     'ì—†', 'ìˆ', 'í•˜', 'ë˜', 'ì•Š',  'í–‰ìœ„',\n",
    "    'ë‚˜', 'ìš°ë¦¬', 'ë„ˆ', 'ë‹¹ì‹ ', 'ê°™', 'ë˜', 'ê²ƒ', 'ë•Œ', 'ë“±', \n",
    "    'ë•Œë¬¸', 'ì •ë„', 'ì‚¬ì‹¤', 'ìƒê°', 'ê²½ìš°', 'ë¬¸ì œ', 'ë°©ë²•', 'ìƒí™©', 'ë‚´ìš©', 'ê²°ê³¼', 'ì‚¬ëŒ',\n",
    "\n",
    "    'í•´ì•¼', 'í•˜ë©´', 'ê²½ìš°', 'ë•ŒëŠ”', 'ì–´ëŠ', 'ë¬´ìŠ¨', 'ì–´ë””', 'ëˆ„êµ¬' , ' ê°€ì§€ë‹¤' , 'ê°€ì§€''í•˜ë‚˜ìš”', 'ìœ„í•´', 'ì´í˜¼'\n",
    "]\n",
    "\n",
    "# # ë²•ë¥  ì „ë¬¸ìš©ì–´ëŠ” ë¬´ì¡°ê±´ í¬í•¨\n",
    "# LEGAL_KEYWORDS = [ 'ìœ„ìë£Œ', 'ì¬ì‚°ë¶„í• ', 'ì–‘ìœ¡ê¶Œ', 'ì¹œê¶Œ', 'ë©´ì ‘êµì„­', \n",
    "#     'í˜‘ì˜ì´í˜¼', 'ì²­êµ¬', 'ë°°ìƒ', 'ì†í•´', 'ì±…ì„',\n",
    "#     'ì°¨ìš©ê¸ˆ', 'ë°˜í™˜', 'ì·¨ì†Œ', 'ì›ìƒíšŒë³µ', 'ì‚¬í•´í–‰ìœ„', 'ì±„ê¶Œì', 'ë°°ìš°ì', 'ì´í˜¼ì‚¬ìœ ',\n",
    "#      'ì´í˜¼', 'ì‚¬ìœ ', \n",
    "#     'í˜¼ì¸', 'ê¸ˆì „ê±°ë˜', 'ì²­êµ¬ê¶Œ',  'ì•¡ìˆ˜', 'ì •í•´ì§€', 'ë¶€ì ë²•',\n",
    "#     'í˜¼ì¸íŒŒíƒ„', 'íŒŒíƒ„', 'ë¶„í• ', 'ì–‘ìœ¡ë¹„', 'ë©´ì ‘',\n",
    "#     'êµì„­', 'í˜‘ì˜', 'ì¡°ì •ì‹ ì²­', 'ì†í•´ë°°ìƒ', 'ë¶€ë¶€', 'ë°°ìš°ì', 'ë‹¹ì‚¬ì', 'ì‚¬ëŒ', 'ê°œì¸', 'ìƒëŒ€ë°©'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "015f1680-4583-4570-b406-966a8e6ec2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):  # ì „ì²˜ë¦¬ í•¨ìˆ˜ \n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜:\n",
    "    1. 'ì œ3ì' í˜•íƒœì˜ ë‹¨ì–´ë¥¼ ì„ì‹œ í† í°ìœ¼ë¡œ ë³´í˜¸/ë³µì›í•˜ì—¬ ìˆ«ìë¥¼ ë³´ì¡´.\n",
    "    2. ë¶ˆí•„ìš”í•œ í•œê¸€ ì´ì™¸ì˜ ë¬¸ìë¥¼ ì œê±°.\n",
    "    3. í˜•íƒœì†Œ ë¶„ì„ ë° ì–´ê°„ ì¶”ì¶œ.\n",
    "    4. ë¶ˆìš©ì–´ ë° 1ê¸€ì ë‹¨ì–´ ì œê±°.\n",
    "    \"\"\"\n",
    "    # 1ë‹¨ê³„: í•œê¸€, ê³µë°±ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°\n",
    "    # re.sub('[^ã„±-ã…ã…-ã…£ê°€-í£ ]', '', text)ëŠ” í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì§€ìš°ë¼ëŠ” ì˜ë¯¸\n",
    "    # ì œìˆ«ì + í•œê¸€ ê¸€ì + ì„ íƒì  ì¡°ì‚¬ê¹Œì§€ í¬í•¨\n",
    "    # ë”•ì…”ë„ˆë¦¬ë¥¼ í•¨ìˆ˜ ë‚´ë¶€ì— ì„ ì–¸í•˜ì—¬ ë§¤ í˜¸ì¶œë§ˆë‹¤ ì´ˆê¸°í™” (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)\n",
    "    protected_matches = {}\n",
    "\n",
    "    def protect_term(match):\n",
    "        # ì°¾ì€ ë¬¸ìì—´(ì˜ˆ: 'ì œ3ì')ì„ ê³ ìœ í•œ í† í°(ì˜ˆ: '###TOKEN0###')ìœ¼ë¡œ ë§¤í•‘\n",
    "        token = f\"###TOKEN{len(protected_matches)}###\"\n",
    "        protected_matches[token] = match.group(0)\n",
    "        return match.group(0) # ì¼ë‹¨ ì›ë³¸ ë‹¨ì–´ ê·¸ëŒ€ë¡œ ë‘” ì±„ë¡œ í˜•íƒœì†Œ ë¶„ì„ ì§„í–‰\n",
    "    def strip_josa(text):\n",
    "    # í•œê¸€ ëª…ì‚¬ ë’¤ â€œì˜â€, â€œê°€â€, â€œì„â€ ë“± ì œê±°\n",
    "        return re.sub(r'([ê°€-í£]+)(ì˜|ê°€|ë¥¼|ì€|ëŠ”|ê³¼|ì™€)$', r'\\1', text)\n",
    "    \n",
    "\n",
    "    # 1ë‹¨ê³„: 'ì œ3ì' í˜•íƒœì˜ ë‹¨ì–´ë¥¼ ì„ì‹œ í† í°ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ë³´í˜¸\n",
    "    re.sub(r'(ì œ\\d+[ê°€-í£]+)', protect_term, text)\n",
    "\n",
    "    # 2ë‹¨ê³„: í•œê¸€, ê³µë°±, ë³´í˜¸ í† í°ì˜ ë¬¸ì(#)ë§Œ ë‚¨ê¸°ê³  ì œê±°\n",
    "    text = re.sub('[^ã„±-ã…ã…-ã…£ê°€-í£# ]', '', text)\n",
    "\n",
    "    # 2ë‹¨ê³„: Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì´ìš©í•œ í† í°í™” ë° ì–´ê°„ ì¶”ì¶œ(Stemming)\n",
    "    # okt.pos(text, stem=True)ëŠ” ë¬¸ì¥ì„ (ë‹¨ì–´, í’ˆì‚¬) í˜•íƒœë¡œ ë‚˜ëˆ„ê³ , 'í•˜ë‹¤', 'ë¨¹ë‹¤'ì²˜ëŸ¼ ì›í˜•ìœ¼ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
    "    # ì˜ˆ: \"ë¨¹ì—ˆì—ˆê³ \" -> ('ë¨¹ë‹¤', 'Verb')\n",
    "    # Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±\n",
    "    okt = Okt()\n",
    "    word_tokens = okt.pos(text, stem=True)\n",
    "\n",
    "    # 3ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±°\n",
    "    # í˜•íƒœì†Œ ë¶„ì„ í›„ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œí•˜ëŠ” í•„í„°ë§ ê³¼ì •\n",
    "    # ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬ ì¤‘ì—ì„œ 1ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    # í’ˆì‚¬ íƒœê·¸ê°€ 'Josa'(ì¡°ì‚¬), 'Eomi'(ì–´ë¯¸), 'Punctuation'(êµ¬ë‘ì ) ë“±ì¸ ë‹¨ì–´ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    meaningful_words = []\n",
    "    for word, pos in word_tokens:\n",
    "        if word in LEGAL_KEYWORDS:\n",
    "            meaningful_words.append(word)\n",
    "        elif pos in ['Noun',' Verb'] and len(word) > 1:\n",
    "            token = strip_josa(word)\n",
    "            if token not in MINIMAL_STOPWORDS:\n",
    "                meaningful_words.append(token)\n",
    "     # 6. ë³´í˜¸ëœ ë‹¨ì–´ ê°•ì œ í¬í•¨ ë° ë³µì› (í•µì‹¬)\n",
    "    # ë³´í˜¸ ëª©ë¡ì— ìˆë˜ ë‹¨ì–´ë“¤ì„ ê°•ì œë¡œ ìµœì¢… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    # (ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì— ë¶„í•´ë˜ì–´ ë“¤ì–´ê°”ì„ ìˆ˜ ìˆì§€ë§Œ, ì™„ë²½í•œ ë³µì›ì„ ìœ„í•´ ê°•ì œ ì¶”ê°€)\n",
    "    for original_term in protected_matches.values():\n",
    "        # 'ì œ3ì' ìì²´ë¥¼ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€\n",
    "        meaningful_words.append(original_term)\n",
    "        \n",
    "    # 7. ì¤‘ë³µ ì œê±° ë° ìµœì¢… ë¬¸ìì—´ ë°˜í™˜\n",
    "    # Setì„ ì´ìš©í•´ ì¤‘ë³µ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    final_words = list(set(meaningful_words))\n",
    "    # ìµœì¢…ì ìœ¼ë¡œ ê³µë°±ìœ¼ë¡œ ì—°ê²°ëœ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤. \n",
    "    # ëª¨ë¸ì— ë”°ë¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(meaningful_words)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "    # í…ìŠ¤íŠ¸ì—ì„œ ë¶„ì„ì— ì˜ë¯¸ ìˆëŠ” í•µì‹¬ ë‹¨ì–´ë“¤ë§Œ ë‚¨ê¸´ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    return ' '.join(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f61de42c-ea4f-44e3-b8c9-1ec0bf78e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. ê° ëª©ì ì— ë§ê²Œ ì „ì²˜ë¦¬ ì»¬ëŸ¼ ìƒì„± ---\n",
    "# ì§ˆë¬¸ ì˜ë„ íŒŒì•… ëª¨ë¸ìš©: 'input' ì»¬ëŸ¼ë§Œ ì „ì²˜ë¦¬\n",
    "# 'input' ì»¬ëŸ¼ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ 'input_processed' ì»¬ëŸ¼ ìƒì„±\n",
    "divorce_df['input_processed'] = divorce_df['input'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "50100ade-6ada-464c-a3f3-974e25400dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "divorce_df['combined_processed'] = divorce_df['text_combined'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "45be4962-433a-4e72-9404-b95ec105d298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì „ì²˜ë¦¬ ì „/í›„ ë¹„êµ ===\n",
      "ì›ë³¸ [0]: í˜¼ì¸ ì¤‘ ë°œìƒí•œ ê¸ˆì „ê±°ë˜ì™€ ê´€ë ¨í•˜ì—¬ ì°¨ìš©ê¸ˆ ë°˜í™˜ ì²­êµ¬ê¶Œì´ ì¸ì •ë˜ëŠ” ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "ê²°ê³¼ [0]: ê±°ë˜ ì°¨ìš© ë°œìƒ ì²­êµ¬ê¶Œ ê´€ë ¨ ì¸ì • ì¡°ê±´ ê¸ˆì „ í˜¼ì¸ ë°˜í™˜\n",
      "\n",
      "ì›ë³¸ [1]: ì œ3ìê°€ ë¶€ë¶€ì˜ ì¼ë°©ê³¼ ë¶€ì •í–‰ìœ„ë¥¼ í•  ê²½ìš° ì–´ë–¤ ë²•ì  ì±…ì„ì„ ì§€ê²Œ ë˜ë‚˜ìš”?\n",
      "ê²°ê³¼ [1]: ë¶€ë¶€ ë¶€ì •í–‰ìœ„ ì±…ì„ ì¼ë°© ë²•ì  ì§€ê²Œ ì œ3ìê°€\n",
      "\n",
      "ì›ë³¸ [2]: ë¯¼ë²• ì œ840ì¡° ì œ1í˜¸ì—ì„œ ê·œì •í•œ ì¬íŒìƒ ì´í˜¼ì‚¬ìœ  ì¤‘ ë¶€ì •í•œ í–‰ìœ„ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
      "ê²°ê³¼ [2]: ê·œì • ì œì¡° ì‚¬ìœ  ì´í˜¼ ì œ840ì¡° ë¯¼ë²• ë¶€ì • ì œ1í˜¸ì—ì„œ ì˜ë¯¸ ì¬íŒ\n",
      "\n",
      "ì›ë³¸ [3]: ë°°ìš°ìê°€ ìˆëŠ” ì‚¬ëŒê³¼ ë¶€ì •í–‰ìœ„ë¥¼ í•œ ê²½ìš° ìœ„ìë£Œì˜ ì•¡ìˆ˜ëŠ” ì–´ë–»ê²Œ ì •í•´ì§€ë‚˜ìš”?\n",
      "ê²°ê³¼ [3]: ë°°ìš°ì ì•¡ìˆ˜ ë¶€ì •í–‰ìœ„ ìœ„ìë£Œ\n",
      "\n",
      "ì›ë³¸ [4]: ì‚¬í•´í–‰ìœ„ ì·¨ì†Œ ë° ì›ìƒíšŒë³µì²­êµ¬ ì†Œì†¡ì—ì„œ ë‹¤ë¥¸ ì±„ê¶Œìì˜ ì²­êµ¬ê°€ ë¶€ì ë²•í•´ì§€ëŠ” ìƒí™©ì€ ì–´ë–¤ ê²½ìš°ì¸ê°€ìš”?\n",
      "ê²°ê³¼ [4]: ë²•í•´ ì†Œì†¡ ì›ìƒíšŒë³µ ë¶€ì  ì²­êµ¬ ì±„ê¶Œì ì·¨ì†Œ ì‚¬í•´í–‰ìœ„ ë‹¤ë¥¸\n",
      "\n",
      "=== ê° ëª©ì ì— ë§ê²Œ ìƒì„±ëœ ì „ì²˜ë¦¬ ì»¬ëŸ¼ë“¤ ===\n",
      "                                               input  \\\n",
      "0    í˜¼ì¸ ì¤‘ ë°œìƒí•œ ê¸ˆì „ê±°ë˜ì™€ ê´€ë ¨í•˜ì—¬ ì°¨ìš©ê¸ˆ ë°˜í™˜ ì²­êµ¬ê¶Œì´ ì¸ì •ë˜ëŠ” ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
      "1          ì œ3ìê°€ ë¶€ë¶€ì˜ ì¼ë°©ê³¼ ë¶€ì •í–‰ìœ„ë¥¼ í•  ê²½ìš° ì–´ë–¤ ë²•ì  ì±…ì„ì„ ì§€ê²Œ ë˜ë‚˜ìš”?   \n",
      "2   ë¯¼ë²• ì œ840ì¡° ì œ1í˜¸ì—ì„œ ê·œì •í•œ ì¬íŒìƒ ì´í˜¼ì‚¬ìœ  ì¤‘ ë¶€ì •í•œ í–‰ìœ„ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?   \n",
      "3         ë°°ìš°ìê°€ ìˆëŠ” ì‚¬ëŒê³¼ ë¶€ì •í–‰ìœ„ë¥¼ í•œ ê²½ìš° ìœ„ìë£Œì˜ ì•¡ìˆ˜ëŠ” ì–´ë–»ê²Œ ì •í•´ì§€ë‚˜ìš”?   \n",
      "4  ì‚¬í•´í–‰ìœ„ ì·¨ì†Œ ë° ì›ìƒíšŒë³µì²­êµ¬ ì†Œì†¡ì—ì„œ ë‹¤ë¥¸ ì±„ê¶Œìì˜ ì²­êµ¬ê°€ ë¶€ì ë²•í•´ì§€ëŠ” ìƒí™©ì€ ì–´...   \n",
      "\n",
      "                       input_processed  \n",
      "0       ê±°ë˜ ì°¨ìš© ë°œìƒ ì²­êµ¬ê¶Œ ê´€ë ¨ ì¸ì • ì¡°ê±´ ê¸ˆì „ í˜¼ì¸ ë°˜í™˜  \n",
      "1             ë¶€ë¶€ ë¶€ì •í–‰ìœ„ ì±…ì„ ì¼ë°© ë²•ì  ì§€ê²Œ ì œ3ìê°€  \n",
      "2  ê·œì • ì œì¡° ì‚¬ìœ  ì´í˜¼ ì œ840ì¡° ë¯¼ë²• ë¶€ì • ì œ1í˜¸ì—ì„œ ì˜ë¯¸ ì¬íŒ  \n",
      "3                      ë°°ìš°ì ì•¡ìˆ˜ ë¶€ì •í–‰ìœ„ ìœ„ìë£Œ  \n",
      "4      ë²•í•´ ì†Œì†¡ ì›ìƒíšŒë³µ ë¶€ì  ì²­êµ¬ ì±„ê¶Œì ì·¨ì†Œ ì‚¬í•´í–‰ìœ„ ë‹¤ë¥¸  \n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ ë¹„êµë¥¼ ìœ„í•´ ì›ë³¸ê³¼ ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ ë‚˜ë€íˆ ì¶œë ¥\n",
    "print(\"\\n=== ì „ì²˜ë¦¬ ì „/í›„ ë¹„êµ ===\")\n",
    "for i in range(5):\n",
    "    print(f\"ì›ë³¸ [{i}]: {divorce_df['input'].iloc[i]}\")\n",
    "    print(f\"ê²°ê³¼ [{i}]: {divorce_df['input_processed'].iloc[i]}\\n\")\n",
    "\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ í™•ì¸\n",
    "print(\"=== ê° ëª©ì ì— ë§ê²Œ ìƒì„±ëœ ì „ì²˜ë¦¬ ì»¬ëŸ¼ë“¤ ===\")\n",
    "print(divorce_df[['input', 'input_processed']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da24447-9fd0-4bed-a327-5110a899ca65",
   "metadata": {},
   "source": [
    "# 2ë‹¨ê³„: ìµœì í™”ëœ TF-IDF ë²¡í„°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "63d24389-f586-4001-9a80-bead82885b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF í–‰ë ¬ í¬ê¸°: (741, 287)\n",
      "ë¬¸ì„œ ìˆ˜: 741\n",
      "íŠ¹ì„±(ë‹¨ì–´) ìˆ˜: 287\n",
      "ìƒìœ„ 10ê°œ íŠ¹ì„±: ['ê°„ì£¼' 'ê°„í†µ' 'ê°œì…' 'ê°œì… ìƒí™œ' 'ê²°ì •' 'ê²°ì • ì†í•´ë°°ìƒ' 'ê²°ì • ìš”ì†Œ' 'ê²°ì • ì •ì‹ ' 'ê³„ì•½' 'ê³ ë ¤']\n",
      "ìƒìœ„ 20ê°œ íŠ¹ì„±: ['ê°„ì£¼' 'ê°„í†µ' 'ê°œì…' 'ê°œì… ìƒí™œ' 'ê²°ì •' 'ê²°ì • ì†í•´ë°°ìƒ' 'ê²°ì • ìš”ì†Œ' 'ê²°ì • ì •ì‹ ' 'ê³„ì•½' 'ê³ ë ¤' 'ê³ ë ¤ ë²•ì›'\n",
      " 'ê³ ë ¤ ì²­êµ¬' 'ê³ í†µ' 'ê³ í†µ ì†í•´ë°°ìƒ' 'ê³ í†µ ì •ì‹ ' 'ê³ í†µ ì²­êµ¬' 'ê³µë™' 'ê´€ê³„' 'ê´€ê³„ ëŒ€í•œ' 'ê´€ê³„ ë¶€ì •í–‰ìœ„']\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TF-IDF Vectorizer ì„¤ì •\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # ìƒìœ„ ë¹ˆë„ 1000ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©\n",
    "    min_df=5,\n",
    "    max_df=0.5,           # 80% ì´ìƒ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì œê±°\n",
    "    lowercase=False,         # ì†Œë¬¸ì ë³€í™˜ x\n",
    "    ngram_range=(1, 2),     # 1-gram, 2-gram ëª¨ë‘ ì‚¬ìš©\n",
    "    sublinear_tf=True,            # TF ê°’ì— ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš© (ì„±ëŠ¥ í–¥ìƒ)\n",
    "    token_pattern=r'[ê°€-í£]{2,}', # í•œê¸€ 2ê¸€ì ì´ìƒ\n",
    ")\n",
    "\n",
    "# TF-IDF í–‰ë ¬ ìƒì„±\n",
    "tfidf_matrix = vectorizer.fit_transform(divorce_df['input_processed'])  # XëŠ” (ë¬¸ì„œ ìˆ˜ Ã— ë‹¨ì–´ ìˆ˜) í¬ê¸°ì˜ í¬ì†Œ í–‰ë ¬\n",
    "\n",
    "print(f\"TF-IDF í–‰ë ¬ í¬ê¸°: {tfidf_matrix.shape}\")\n",
    "print(f\"ë¬¸ì„œ ìˆ˜: {tfidf_matrix.shape[0]}\")\n",
    "print(f\"íŠ¹ì„±(ë‹¨ì–´) ìˆ˜: {tfidf_matrix.shape[1]}\")\n",
    "\n",
    "# íŠ¹ì„± ì´ë¦„(ë‹¨ì–´ë“¤) í™•ì¸\n",
    "# vectorizer.get_feature_names_out()ë¡œ ì–´ë–¤ ë‹¨ì–´ê°€ ë²¡í„°ì— ë“¤ì–´ê°”ëŠ”ì§€ í™•ì¸ ê°€ëŠ¥\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"ìƒìœ„ 10ê°œ íŠ¹ì„±: {feature_names[:10]}\")\n",
    "print(f\"ìƒìœ„ 20ê°œ íŠ¹ì„±: {feature_names[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f42c30-d2ed-44f0-9729-57c401649768",
   "metadata": {},
   "source": [
    "# 3ë‹¨ê³„: í† í”½ ê°œìˆ˜ ìµœì í™”\n",
    "# ìµœì ì˜ í† í”½ ê°œìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•´ ì—¬ëŸ¬ ì§€í‘œë¥¼ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b565c218-e853-4461-8257-dd99312a61fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í”½ ìˆ˜ 2: Perplexity = 35462.0094\n",
      "í† í”½ ìˆ˜ 3: Perplexity = 43615.2530\n",
      "í† í”½ ìˆ˜ 4: Perplexity = 60242.6583\n",
      "í† í”½ ìˆ˜ 5: Perplexity = 57516.5650\n",
      "í† í”½ ìˆ˜ 6: Perplexity = 63302.6877\n",
      "í† í”½ ìˆ˜ 7: Perplexity = 78152.4240\n",
      "í† í”½ ìˆ˜ 8: Perplexity = 80238.3525\n",
      "í† í”½ ìˆ˜ 9: Perplexity = 95862.0042\n",
      "í† í”½ ìˆ˜ 10: Perplexity = 84849.3090\n",
      "ìµœì ì˜ í† í”½ ìˆ˜ (Perplexity ê¸°ì¤€): 2\n"
     ]
    }
   ],
   "source": [
    "# 4-1. LDA Perplexity ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\n",
    "\n",
    "# í† í”½ ê°œìˆ˜ë³„ Perplexity ê³„ì‚°\n",
    "perplexity_scores = []\n",
    "topic_range = range(2, 11)  # 2~10ê°œ í† í”½ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "for n_topics in topic_range:\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics, \n",
    "        random_state=42,\n",
    "        max_iter=100,\n",
    "        doc_topic_prior=0.1,      # ë¬¸ì„œ-í† í”½ ë¶„í¬ ì¡°ì •\n",
    "        topic_word_prior=0.01     # í† í”½-ë‹¨ì–´ ë¶„í¬ ì¡°ì •\n",
    "    )\n",
    "    lda.fit(tfidf_matrix)\n",
    "    perplexity = lda.perplexity(tfidf_matrix)\n",
    "    perplexity_scores.append(perplexity)\n",
    "    print(f\"í† í”½ ìˆ˜ {n_topics}: Perplexity = {perplexity:.4f}\")\n",
    "\n",
    "# ìµœì ì˜ í† í”½ ìˆ˜ ì„ íƒ\n",
    "optimal_topics = topic_range[np.argmin(perplexity_scores)]\n",
    "print(f\"ìµœì ì˜ í† í”½ ìˆ˜ (Perplexity ê¸°ì¤€): {optimal_topics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51264159-071b-448a-b0cf-594b49990709",
   "metadata": {},
   "source": [
    "# ë” ì •í™•í•œ í† í”½ ëª¨ë¸ë§ì„ ìœ„í•´ Gensim ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©\n",
    "# ê³ ê¸‰ ê¸°ë²•: Gensim LDA + Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "42001eee-6a3a-4fc4-aab4-fee5f56be87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í”½ ìˆ˜ 2: Coherence = 0.3097\n",
      "í† í”½ ìˆ˜ 3: Coherence = 0.3209\n",
      "í† í”½ ìˆ˜ 4: Coherence = 0.3330\n",
      "í† í”½ ìˆ˜ 5: Coherence = 0.3446\n",
      "í† í”½ ìˆ˜ 6: Coherence = 0.3491\n",
      "í† í”½ ìˆ˜ 7: Coherence = 0.3603\n",
      "í† í”½ ìˆ˜ 8: Coherence = 0.3724\n",
      "í† í”½ ìˆ˜ 9: Coherence = 0.3339\n",
      "í† í”½ ìˆ˜ 10: Coherence = 0.3382\n",
      "í† í”½ ìˆ˜ 11: Coherence = 0.3442\n",
      "\n",
      "Coherence ìƒìœ„ 3ê°œ: [6, 7, 8]\n",
      "\n",
      "=== 6ê°œ í† í”½ ê²°ê³¼ ===\n",
      "í† í”½ 0: ì†í•´, ì •ì‹ , ë°°ìƒ, ì…ì¦, ë¶€ì •í–‰ìœ„, ì ìš©, ì²­êµ¬, ëŒ€í•œ\n",
      "í† í”½ 1: ë¶€ë¶€, ìƒí™œ, ì œ3ìê°€, ì±…ì„, ì¡°ê±´, í˜¼ì¸, ì¹¨í•´, ë²•ì \n",
      "í† í”½ 2: ì²­êµ¬, ìœ„ìë£Œ, ë¶€ì •í–‰ìœ„, ì†í•´ë°°ìƒ, ëŒ€í•œ, ê¸°ì¤€, ë²”ìœ„, ì¸ì •\n",
      "í† í”½ 3: ì¬ì‚°, ë¶„í• , ì´í˜¼, ì²­êµ¬, ê³ ë ¤, ìš”ì†Œ, ì–‘ìœ¡ë¹„, íŒë‹¨\n",
      "í† í”½ 4: ë¶€ì •í–‰ìœ„, ì†í•´ë°°ìƒ, ë°°ìš°ì, ì±…ì„, ì œ3ìê°€, ì¡°ê±´, ë¶€ë¶€, ì¼ë°©\n",
      "í† í”½ 5: ê´€ê³„, í˜¼ì¸, ì¸ì •, ë¶€ì •, ì‚¬ì‹¤í˜¼, ë°°ìš°ì, íŒŒíƒ„, ì¡°ê±´\n",
      "\n",
      "=== 7ê°œ í† í”½ ê²°ê³¼ ===\n",
      "í† í”½ 0: ì •ì‹ , ì†í•´ë°°ìƒ, ë¶€ì •í–‰ìœ„, ê³ í†µ, ì²­êµ¬, ì†í•´, ë°°ìƒ, ë°°ìš°ì\n",
      "í† í”½ 1: ì¬ì‚°, ë¶„í• , ì´í˜¼, ì¡°ê±´, ë¶€ë¶€, ì‚¬í•´í–‰ìœ„, íš¨ë ¥, ì¶”ì •\n",
      "í† í”½ 2: ì²­êµ¬, ìœ„ìë£Œ, ë¶€ì •í–‰ìœ„, ì†í•´ë°°ìƒ, ë²”ìœ„, ê¸°ì¤€, ê²°ì •, ëŒ€í•œ\n",
      "í† í”½ 3: ìš”ì†Œ, ê³ ë ¤, ì²­êµ¬, ì–‘ìœ¡ë¹„, íŒë‹¨, ë²•ì›, ëŒ€í•œ, ê¸°ì¤€\n",
      "í† í”½ 4: ë¶€ì •í–‰ìœ„, ë°°ìš°ì, ì†í•´ë°°ìƒ, ì¡°ê±´, ì±…ì„, ì²­êµ¬, ë²•ì , ë¶€ë¶€\n",
      "í† í”½ 5: ê´€ê³„, í˜¼ì¸, ë¶€ì •, ë°°ìš°ì, ì‚¬ì‹¤í˜¼, ì¸ì •, íŒŒíƒ„, ë¶ˆë²•í–‰ìœ„\n",
      "í† í”½ 6: ë¶€ë¶€, ì œ3ìê°€, ë¶€ì •í–‰ìœ„, ì±…ì„, ìƒí™œ, ë¶ˆë²•í–‰ìœ„, ì¡°ê±´, ì¼ë°©\n",
      "\n",
      "=== 8ê°œ í† í”½ ê²°ê³¼ ===\n",
      "í† í”½ 0: ì†í•´, ë°°ìƒ, ì •ì‹ , ë°œìƒ, ëŒ€í•œ, ê³ í†µ, ê´€ê³„, í•´ì†Œ\n",
      "í† í”½ 1: ì¬ì‚°, ë¶„í• , ì´í˜¼, ì¡°ê±´, íš¨ë ¥, ì‚¬í•´í–‰ìœ„, ë¶€ë¶€, ì¸ì •\n",
      "í† í”½ 2: ì²­êµ¬, ì†í•´ë°°ìƒ, ë¶€ì •í–‰ìœ„, ìœ„ìë£Œ, ë²”ìœ„, ê²°ì •, ê¸°ì¤€, ì†Œì†¡\n",
      "í† í”½ 3: ê³ ë ¤, ìš”ì†Œ, ì²­êµ¬, ì–‘ìœ¡ë¹„, ê¸°ì¤€, íŒë‹¨, ë²•ì›, ìœ„ìë£Œ\n",
      "í† í”½ 4: ë¶€ì •í–‰ìœ„, ë°°ìš°ì, ì†í•´ë°°ìƒ, ì •ì‹ , ì±…ì„, ì¡°ê±´, ê³ í†µ, ì²­êµ¬\n",
      "í† í”½ 5: ê´€ê³„, í˜¼ì¸, ë¶€ì •, ë°°ìš°ì, ì‚¬ì‹¤í˜¼, ì¸ì •, íŒŒíƒ„, ì¡°ê±´\n",
      "í† í”½ 6: ë¶€ë¶€, ì±…ì„, ì œ3ìê°€, ë¶€ì •í–‰ìœ„, ì¡°ê±´, ìƒí™œ, ì†í•´ë°°ìƒ, ë²•ì \n",
      "í† í”½ 7: ë¶€ë¶€, ë¶€ì •í–‰ìœ„, ì¹¨í•´, ìƒí™œ, ë¶ˆë²•í–‰ìœ„, ì œ3ìê°€, ì¸ì •, í˜¼ì¸\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "texts = [text.split() for text in divorce_df['input_processed']]\n",
    "\n",
    "# Dictionary ìƒì„±\n",
    "dictionary = Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "\n",
    "# Corpus ìƒì„±  \n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# ìµœì  í† í”½ ìˆ˜ ì°¾ê¸° (Coherence Score ì‚¬ìš©)\n",
    "coherence_scores = []\n",
    "models_dict = {}\n",
    "topic_range = range(2, 12)\n",
    "\n",
    "for num_topics in topic_range:\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary, \n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=20,\n",
    "        alpha='auto',        # ìë™ ì¡°ì •\n",
    "        per_word_topics=True\n",
    "        \n",
    "    )\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        model=lda_model, \n",
    "        texts=texts, \n",
    "        dictionary=dictionary, \n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "    models_dict[num_topics] = lda_model\n",
    "    print(f\"í† í”½ ìˆ˜ {num_topics}: Coherence = {coherence_score:.4f}\")\n",
    "\n",
    "# ìƒìœ„ 3ê°œ í›„ë³´ ì„ ì •\n",
    "top_3_indices = np.argsort(coherence_scores)[-3:]\n",
    "top_3_topics = [topic_range[i] for i in top_3_indices]\n",
    "print(f\"\\nCoherence ìƒìœ„ 3ê°œ: {top_3_topics}\")\n",
    "\n",
    "\n",
    "# ê° í›„ë³´ì˜ ì‹¤ì œ í† í”½ ë‚´ìš© í™•ì¸\n",
    "for num_topics in top_3_topics:\n",
    "    print(f\"\\n=== {num_topics}ê°œ í† í”½ ê²°ê³¼ ===\")\n",
    "    model = models_dict[num_topics]\n",
    "    \n",
    "    for topic_id in range(num_topics):\n",
    "        terms = model.show_topic(topic_id, topn=8)\n",
    "        words = [term[0] for term in terms]\n",
    "        print(f\"í† í”½ {topic_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ce1ca-30ad-48e3-886f-131b3b63ffe5",
   "metadata": {},
   "source": [
    "# 4ë‹¨ê³„: LDA í† í”½ ëª¨ë¸ë§ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2d6e19a8-38d9-4b66-8c59-1329f7d735d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA ê²°ê³¼ í˜•íƒœ: (741, 5)\n"
     ]
    }
   ],
   "source": [
    "# ìµœì ì˜ í† í”½ ìˆ˜ë¡œ LDA ëª¨ë¸ ìƒì„± \n",
    "final_lda = LatentDirichletAllocation(\n",
    "    n_components=5,    # ìµœì  í† í”½ ìˆ˜ ì‚¬ìš©\n",
    "    random_state=42,\n",
    "    max_iter=100,\n",
    "    learning_method='batch',         # ë˜ëŠ” 'online'\n",
    "    doc_topic_prior=0.1,        # ë¬¸ì„œ-í† í”½ ë¶„í¬ ì¡°ì •\n",
    "    topic_word_prior=0.01     # í† í”½-ë‹¨ì–´ ë¶„í¬ ì¡°ì •\n",
    ")\n",
    "\n",
    "# LDA ëª¨ë¸ í›ˆë ¨ ë° í† í”½ í™•ë¥  ê³„ì‚°\n",
    "lda_result = final_lda.fit_transform(tfidf_matrix)\n",
    "\n",
    "# ê° ë¬¸ì„œì˜ ì£¼ìš” í† í”½ ê²°ì • (í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í”½)\n",
    "topic_assignments = np.argmax(lda_result, axis=1)\n",
    "\n",
    "print(f\"LDA ê²°ê³¼ í˜•íƒœ: {lda_result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84345c-6335-4003-aa2e-69677b53f8b4",
   "metadata": {},
   "source": [
    "# 5ë‹¨ê³„: í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f60c1753-e0f6-456b-8c7a-88e12d2926d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ===\n",
      "\n",
      "í† í”½ 0:\n",
      "  ì´í˜¼: 17.2834\n",
      "  ì¬ì‚°: 11.8825\n",
      "  ì˜ë¬´: 11.4772\n",
      "  ë¶„í• : 10.9563\n",
      "  ì¡°ê±´: 9.3630\n",
      "  ìš”ì†Œ: 8.4846\n",
      "  ì–‘ìœ¡ë¹„: 7.9223\n",
      "  ì²­êµ¬: 7.8003\n",
      "  ì§€ê¸‰: 7.5813\n",
      "  ì†Œì†¡: 6.8999\n",
      "\n",
      "í† í”½ 1:\n",
      "  ë¶€ì •í–‰ìœ„: 25.4262\n",
      "  ì¡°ê±´: 25.3586\n",
      "  ì†í•´ë°°ìƒ: 23.2311\n",
      "  ë¶€ì •í–‰ìœ„ ì¡°ê±´: 21.1535\n",
      "  ì±…ì„: 17.7814\n",
      "  ë°°ìš°ì: 16.5995\n",
      "  ì •ì‹ : 15.2337\n",
      "  ì¸ì •: 14.7464\n",
      "  ìê°€: 14.6135\n",
      "  ì²­êµ¬: 13.4194\n",
      "\n",
      "í† í”½ 2:\n",
      "  ì¬ì‚°: 11.5920\n",
      "  ë²”ìœ„: 10.3312\n",
      "  ë¶„í• : 9.8304\n",
      "  íš¨ë ¥: 9.7343\n",
      "  ê´€ê³„: 9.1080\n",
      "  ì‚¬ì‹¤í˜¼: 8.8724\n",
      "  ê¸°ì¤€: 7.7579\n",
      "  íŒë‹¨: 7.4158\n",
      "  ì´í˜¼ì†Œì†¡: 7.1094\n",
      "  ë²•ë¥ : 6.5353\n",
      "\n",
      "í† í”½ 3:\n",
      "  ë¶€ì •í–‰ìœ„: 21.9430\n",
      "  ì†í•´ë°°ìƒ: 19.1830\n",
      "  ìœ„ìë£Œ: 17.6841\n",
      "  ë°°ìš°ì: 17.6053\n",
      "  ì²­êµ¬: 17.0722\n",
      "  ê¸°ì¤€: 16.3779\n",
      "  ë°°ìš°ì ë¶€ì •í–‰ìœ„: 13.2264\n",
      "  ì •ì‹ : 12.8199\n",
      "  ë¶€ì •í–‰ìœ„ ê¸°ì¤€: 12.6538\n",
      "  ëŒ€í•œ: 11.5298\n",
      "\n",
      "í† í”½ 4:\n",
      "  ìƒí™œ: 16.3974\n",
      "  ë¶€ë¶€: 15.4108\n",
      "  ìê°€: 11.6718\n",
      "  ì¹¨í•´: 10.9257\n",
      "  ì±…ì„: 9.7229\n",
      "  í˜¼ì¸: 9.4150\n",
      "  ë¶ˆë²•í–‰ìœ„: 9.1919\n",
      "  ë¶€ë¶€ ë¶ˆë²•í–‰ìœ„: 8.7234\n",
      "  í•˜ë‚˜ìš”: 8.5515\n",
      "  ì¼ë°©: 6.7546\n"
     ]
    }
   ],
   "source": [
    "# í† í”½ë³„ ëŒ€í‘œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "print(\"=== í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ===\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(final_lda.components_):\n",
    "    # ê° í† í”½ì—ì„œ ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "    top_words_idx = topic.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    top_weights = [topic[i] for i in top_words_idx]\n",
    "    \n",
    "    print(f\"\\ní† í”½ {topic_idx}:\")\n",
    "    for word, weight in zip(top_words, top_weights):\n",
    "        print(f\"  {word}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5da37e8e-84e2-4ee7-90b4-c4fbbec89df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA ê²°ê³¼ í˜•íƒœ: (741, 6)\n"
     ]
    }
   ],
   "source": [
    "# ìµœì ì˜ í† í”½ ìˆ˜ë¡œ LDA ëª¨ë¸ ìƒì„± \n",
    "final_lda = LatentDirichletAllocation(\n",
    "    n_components=6,    # ìµœì  í† í”½ ìˆ˜ ì‚¬ìš©\n",
    "    random_state=42,\n",
    "    max_iter=100,\n",
    "    learning_method='batch',         # ë˜ëŠ” 'online'\n",
    "    doc_topic_prior=0.1,        # ë¬¸ì„œ-í† í”½ ë¶„í¬ ì¡°ì •\n",
    "    topic_word_prior=0.01     # í† í”½-ë‹¨ì–´ ë¶„í¬ ì¡°ì •\n",
    ")\n",
    "\n",
    "# LDA ëª¨ë¸ í›ˆë ¨ ë° í† í”½ í™•ë¥  ê³„ì‚°\n",
    "lda_result = final_lda.fit_transform(tfidf_matrix)\n",
    "\n",
    "# ê° ë¬¸ì„œì˜ ì£¼ìš” í† í”½ ê²°ì • (í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í”½)\n",
    "topic_assignments = np.argmax(lda_result, axis=1)\n",
    "\n",
    "print(f\"LDA ê²°ê³¼ í˜•íƒœ: {lda_result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "90d5d95b-1923-452a-a36d-b7c3a2a91244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ===\n",
      "\n",
      "í† í”½ 0:\n",
      "  ì˜ë¬´: 13.4350\n",
      "  ì§€ê¸‰: 9.6573\n",
      "  ë²•ì : 8.0409\n",
      "  ì–‘ìœ¡ë¹„: 6.7887\n",
      "  ì•½ì •: 5.5690\n",
      "  ê·¼ê±°: 5.4483\n",
      "  ëŒ€í•œ: 5.4039\n",
      "  ëŒ€í•œ ê¸°ì¤€: 5.1556\n",
      "  ê´€ë ¨: 5.1415\n",
      "  ë¶€ë¶€ ì˜ë¬´: 4.5447\n",
      "\n",
      "í† í”½ 1:\n",
      "  ì¡°ê±´: 26.7358\n",
      "  ë¶€ì •í–‰ìœ„ ì¡°ê±´: 21.1535\n",
      "  ë¶€ì •í–‰ìœ„: 20.0830\n",
      "  ì†í•´ë°°ìƒ: 16.6737\n",
      "  ì±…ì„: 14.6157\n",
      "  ë°°ìš°ì: 13.3136\n",
      "  ìê°€: 12.6087\n",
      "  ë°°ìš°ì ë¶€ì •í–‰ìœ„: 12.4383\n",
      "  ì¼ë°©: 10.8040\n",
      "  ì¡°ê±´ ì±…ì„: 10.5110\n",
      "\n",
      "í† í”½ 2:\n",
      "  í˜¼ì¸: 12.4904\n",
      "  ì¹¨í•´: 10.9257\n",
      "  ê´€ê³„: 9.6127\n",
      "  ìƒí™œ: 8.6037\n",
      "  íš¨ë ¥: 7.3829\n",
      "  ë²•ë¥ : 6.4168\n",
      "  ìƒí™œ ì¹¨í•´: 6.1245\n",
      "  ë¶€ë¶€: 5.9249\n",
      "  íŒŒíƒ„: 5.4593\n",
      "  í˜¼ì¸ ì†í•´ë°°ìƒ: 5.2935\n",
      "\n",
      "í† í”½ 3:\n",
      "  ì†í•´ë°°ìƒ: 25.4766\n",
      "  ë¶€ì •í–‰ìœ„: 24.1695\n",
      "  ì²­êµ¬: 21.2353\n",
      "  ë°°ìš°ì: 19.3219\n",
      "  ì •ì‹ : 18.2584\n",
      "  ì²­êµ¬ ì†í•´ë°°ìƒ: 18.2321\n",
      "  ìœ„ìë£Œ: 17.4927\n",
      "  ê¸°ì¤€: 17.1495\n",
      "  ê³ í†µ: 15.1484\n",
      "  ì†í•´ë°°ìƒ ì •ì‹ : 14.4894\n",
      "\n",
      "í† í”½ 4:\n",
      "  ë¶€ë¶€: 13.7209\n",
      "  ìê°€: 10.3482\n",
      "  ë²•ì : 8.5741\n",
      "  ì±…ì„: 8.4037\n",
      "  ë°œìƒ: 8.3516\n",
      "  ëŒ€í•´: 8.2373\n",
      "  ìƒí™œ: 7.8038\n",
      "  ë¶€ë¶€ ë¶ˆë²•í–‰ìœ„: 7.4963\n",
      "  ë¶€ì •í–‰ìœ„ ì¼ë°©: 7.2992\n",
      "  ì¼ë°©: 7.0558\n",
      "\n",
      "í† í”½ 5:\n",
      "  ì¬ì‚°: 23.4646\n",
      "  ë¶„í• : 20.7766\n",
      "  ì´í˜¼: 19.1640\n",
      "  ì‚¬ì‹¤í˜¼: 8.4449\n",
      "  ì ìš©: 7.3631\n",
      "  ì¡°ê±´: 7.2685\n",
      "  ì†Œì†¡: 6.8551\n",
      "  ì²­êµ¬: 6.5493\n",
      "  ì‚¬í•´í–‰ìœ„: 6.4323\n",
      "  ê³ ë ¤: 6.1148\n"
     ]
    }
   ],
   "source": [
    "# í† í”½ë³„ ëŒ€í‘œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "print(\"=== í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ===\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(final_lda.components_):\n",
    "    # ê° í† í”½ì—ì„œ ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "    top_words_idx = topic.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    top_weights = [topic[i] for i in top_words_idx]\n",
    "    \n",
    "    print(f\"\\ní† í”½ {topic_idx}:\")\n",
    "    for word, weight in zip(top_words, top_weights):\n",
    "        print(f\"  {word}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28907455-4af9-40bf-a94c-dbf54cd33c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì ì˜ í† í”½ ìˆ˜ë¡œ LDA ëª¨ë¸ ìƒì„± \n",
    "final_lda = LatentDirichletAllocation(\n",
    "    n_components=5,    # ìµœì  í† í”½ ìˆ˜ ì‚¬ìš©\n",
    "    random_state=42,\n",
    "    max_iter=100,\n",
    "    learning_method='batch',         # ë˜ëŠ” 'online'\n",
    "    doc_topic_prior=0.1,        # ë¬¸ì„œ-í† í”½ ë¶„í¬ ì¡°ì •\n",
    "    topic_word_prior=0.01     # í† í”½-ë‹¨ì–´ ë¶„í¬ ì¡°ì •\n",
    ")\n",
    "\n",
    "# LDA ëª¨ë¸ í›ˆë ¨ ë° í† í”½ í™•ë¥  ê³„ì‚°\n",
    "lda_result = final_lda.fit_transform(tfidf_matrix)\n",
    "\n",
    "# ê° ë¬¸ì„œì˜ ì£¼ìš” í† í”½ ê²°ì • (í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í”½)\n",
    "topic_assignments = np.argmax(lda_result, axis=1)\n",
    "\n",
    "print(f\"LDA ê²°ê³¼ í˜•íƒœ: {lda_result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5981d3-9ba5-4012-8337-a5cb6ff546e8",
   "metadata": {},
   "source": [
    "# 7ë‹¨ê³„: ìµœì¢… í† í”½ ë¼ë²¨ í• ë‹¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3daed17d-2b5f-417c-94ef-c091cb35519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… í† í”½ ë¼ë²¨ ë¶„í¬:\n",
      "topic_label\n",
      "1    419\n",
      "0    322\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== í† í”½ 0 ë¬¸ì„œ ì˜ˆì‹œ ===\n",
      "1. ë¶€ë¶€ ì–´ë–»ë‹¤ ë¶€ì •í–‰ìœ„ ì¼ë°© ì±…ì„ ë²•ì  ì§€ê²Œ ì œ3ìê°€...\n",
      "2. ìœ„ìë£Œ ë°°ìš°ì ì•¡ìˆ˜ ì •í•´ì§€ë‹¤ ë¶€ì •í–‰ìœ„ ì–´ë–»ë‹¤ ì‚¬ëŒê³¼...\n",
      "3. ë°°ìš°ì ì–´ë–»ë‹¤ ë¶€ì •í–‰ìœ„ ì±…ì„ ì‚¬ëŒê³¼ ì œ3ìëŠ” ë²•ì ...\n",
      "\n",
      "=== í† í”½ 1 ë¬¸ì„œ ì˜ˆì‹œ ===\n",
      "1. ê±°ë˜ ì°¨ìš© ë°œìƒ ì²­êµ¬ê¶Œ ê´€ë ¨ ì¸ì • ì¡°ê±´ ê¸ˆì „ í˜¼ì¸ ë°˜í™˜...\n",
      "2. ê·œì • ì œì¡° ì‚¬ìœ  ì´í˜¼ ì œ840ì¡° ë¯¼ë²• ë¶€ì • í–‰ìœ„ ì œ1í˜¸ì—ì„œ ì œí˜¸ ì˜ë¯¸ ì¬íŒ...\n",
      "3. ë²•í•´ ì†Œì†¡ ì–´ë–»ë‹¤ ì›ìƒíšŒë³µ ë¶€ì  ì²­êµ¬ ì±„ê¶Œì ì·¨ì†Œ ì‚¬í•´í–‰ìœ„ ë‹¤ë¥¸...\n"
     ]
    }
   ],
   "source": [
    "# topic_label ì»¬ëŸ¼ì— ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "divorce_df['topic_label'] = topic_assignments\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"ìµœì¢… í† í”½ ë¼ë²¨ ë¶„í¬:\")\n",
    "print(divorce_df['topic_label'].value_counts())\n",
    "\n",
    "# í† í”½ë³„ ë¬¸ì„œ ì˜ˆì‹œ í™•ì¸\n",
    "for topic_idx in range(2):\n",
    "    print(f\"\\n=== í† í”½ {topic_idx} ë¬¸ì„œ ì˜ˆì‹œ ===\")\n",
    "    topic_docs = divorce_df[divorce_df['topic_label'] == topic_idx]['input_processed']\n",
    "    for i, doc in enumerate(topic_docs.head(3)):\n",
    "        print(f\"{i+1}. {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cf2eb-1a66-49fc-bfcd-a0491221fdaa",
   "metadata": {},
   "source": [
    "# 8ë‹¨ê³„: K-Means í´ëŸ¬ìŠ¤í„°ë§ (ë¹„êµ ë¶„ì„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805515ec-7644-4f93-b153-bf1995a46291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Meansë¡œë„ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (ë¹„êµìš©)\n",
    "kmeans_model = KMeans(\n",
    "    n_clusters=2,\n",
    "    random_state=42,\n",
    "    max_iter=300,\n",
    "    n_init=10  # ì•ˆì •í™”\n",
    ")\n",
    "\n",
    "kmeans_labels = kmeans_model.fit_predict(tfidf_matrix)\n",
    "divorce_df['topic_label_kmeans'] = kmeans_labels\n",
    "\n",
    "# LDAì™€ K-Means ê²°ê³¼ ë¹„êµ\n",
    "comparison_df = pd.DataFrame({\n",
    "    'document_id': range(len(divorce_df)),\n",
    "    'lda_topic': topic_assignments,\n",
    "    'kmeans_cluster': kmeans_labels,\n",
    "    'input_text': divorce_df['input_processed'].apply(lambda x: x[:50] + '...' if len(x) > 50 else x)\n",
    "})\n",
    "\n",
    "print(\"LDA vs K-Means ê²°ê³¼ ë¹„êµ:\")\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebf43f-2d45-44c9-aa79-743e267b39ff",
   "metadata": {},
   "source": [
    "# 9ë‹¨ê³„: ê²°ê³¼ í•´ì„ ë° í† í”½ ëª…ëª…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74e9f32f-513b-49f8-9db4-76ec01111374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== í† í”½ë³„ ìƒì„¸ ë¶„ì„ ===\n",
      "\n",
      "í† í”½ 0 (ë¬¸ì„œ 322ê°œ):\n",
      "  ë¶€ì •í–‰ìœ„: 15.2116\n",
      "  ì–´ë–»ë‹¤: 12.3022\n",
      "  ìœ„ìë£Œ: 11.6667\n",
      "  ì²­êµ¬: 11.5418\n",
      "  ë°°ìš°ì: 11.4851\n",
      "  ì†í•´ë°°ìƒ: 10.3333\n",
      "  ë¶€ë¶€: 9.3836\n",
      "  ì •ì‹ : 8.3524\n",
      "  ì¸í•˜ë‹¤: 8.3454\n",
      "  ì¡°ê±´: 8.1660\n",
      "\n",
      "í† í”½ 1 (ë¬¸ì„œ 419ê°œ):\n",
      "  ë¶€ì •í–‰ìœ„: 21.0835\n",
      "  ì†í•´ë°°ìƒ: 20.8283\n",
      "  ì¸í•˜ë‹¤: 16.9923\n",
      "  ì²­êµ¬: 16.3750\n",
      "  ì¡°ê±´: 16.1127\n",
      "  ì±…ì„: 15.6096\n",
      "  ì¸ì •: 15.5422\n",
      "  ì–´ë–»ë‹¤: 15.4694\n",
      "  ë¶€ë¶€: 13.6787\n",
      "  ì¸í•˜ë‹¤ ì†í•´ë°°ìƒ: 12.4126\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== í† í”½ë³„ ìƒì„¸ ë¶„ì„ ===\")\n",
    "for topic_idx, topic in enumerate(final_lda.components_):\n",
    "        top_words_idx = topic.argsort()[-15:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_weights = [topic[i] for i in top_words_idx]\n",
    "\n",
    "        print(f\"\\ní† í”½ {topic_idx} (ë¬¸ì„œ {sum(topic_assignments == topic_idx)}ê°œ):\")\n",
    "        for word, weight in zip(top_words[:10], top_weights[:10]):\n",
    "            print(f\"  {word}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4fce3d9-0fef-4b21-b492-0dd190a85639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ì „ì²´ ë°ì´í„°í”„ë ˆì„ ì¬ì¡°í•©\n",
    "final_df = pd.concat([divorce_df, non_divorce_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c8d46dd-5128-4cb4-a995-f6bf2a888264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ ìµœì¢… í† í”½ ë¶„í¬:\n",
      "  í† í”½ -1 (ë¯¸ë¶„ë¥˜): 3706ê°œ\n",
      "  í† í”½ 0: 322ê°œ\n",
      "  í† í”½ 1: 419ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nğŸ“ˆ ìµœì¢… í† í”½ ë¶„í¬:\")\n",
    "topic_counts = final_df['topic_label'].value_counts().sort_index()\n",
    "for topic_id, count in topic_counts.items():\n",
    "    if topic_id == -1:\n",
    "        print(f\"  í† í”½ {topic_id} (ë¯¸ë¶„ë¥˜): {count}ê°œ\")\n",
    "    else:\n",
    "        print(f\"  í† í”½ {topic_id}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e9806-d5a2-4d7f-a615-c27644138ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í”½ë³„ í•´ì„ (ìˆ˜ë™ìœ¼ë¡œ ì˜ë¯¸ë¥¼ ë¶€ì—¬)\n",
    "topic_interpretation = {}\n",
    "\n",
    "num_topics = len([t for t in topic_counts.index if t != -1])\n",
    "\n",
    "if num_topics == 2:\n",
    "        topic_interpretation = {\n",
    "            0: \"ì¬ì‚°ë¶„í•  ê´€ë ¨\",\n",
    "            1: \"ì–‘ìœ¡ê¶Œ/ì¹œê¶Œ ê´€ë ¨\", \n",
    "            -1: \"ë¹„ì´í˜¼ ê´€ë ¨\"\n",
    "        }\n",
    "# elif num_topics == 4:\n",
    "#         topic_interpretation = {\n",
    "#             0: \"ì¬ì‚°ë¶„í•  ê´€ë ¨\",\n",
    "#             1: \"ì–‘ìœ¡ê¶Œ/ì¹œê¶Œ ê´€ë ¨\",\n",
    "#             2: \"ìœ„ìë£Œ ê´€ë ¨\",\n",
    "#             3: \"ì´í˜¼ì ˆì°¨ ê´€ë ¨\",\n",
    "#             -1: \"ë¹„ì´í˜¼ ê´€ë ¨\"\n",
    "#         }\n",
    "# elif num_topics == 5:\n",
    "#         topic_interpretation = {\n",
    "#             0: \"ì¬ì‚°ë¶„í•  ê´€ë ¨\",\n",
    "#             1: \"ì–‘ìœ¡ê¶Œ/ì¹œê¶Œ ê´€ë ¨\", \n",
    "#             2: \"ìœ„ìë£Œ ê´€ë ¨\",\n",
    "#             3: \"ì´í˜¼ì ˆì°¨ ê´€ë ¨\",\n",
    "#             4: \"ê¸°íƒ€ ì´í˜¼ ë²•ë¥ \",\n",
    "#             -1: \"ë¹„ì´í˜¼ ê´€ë ¨\"\n",
    "#         }\n",
    "else:\n",
    "        # ê¸°ë³¸ í•´ì„\n",
    "        for i in range(num_topics):\n",
    "            topic_interpretation[i] = f\"ì´í˜¼ í† í”½ {i}\"\n",
    "        topic_interpretation[-1] = \"ë¹„ì´í˜¼ ê´€ë ¨\"\n",
    "\n",
    "# í•´ì„ ì¶”ê°€\n",
    "final_df['topic_interpretation'] = final_df['topic_label'].map(topic_interpretation)\n",
    "\n",
    "print(f\"\\nğŸ¯ í† í”½ í•´ì„:\")\n",
    "for topic_id, interpretation in topic_interpretation.items():\n",
    "        count = topic_counts.get(topic_id, 0)\n",
    "        print(f\"  í† í”½ {topic_id}: {interpretation} ({count}ê°œ)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85e20f-16e5-4861-a369-0d87ca8ce1d6",
   "metadata": {},
   "source": [
    "# 10ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° í™œìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c69ee-5cf4-45a8-a6b7-f97f7b835eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"\\n=== ìµœì¢… ê²°ê³¼ í™•ì¸ ===\")\n",
    "print(final_df[['is_divorce', 'topic_label', 'topic_interpretation']].value_counts())\n",
    "\n",
    "# CSV ì €ì¥\n",
    "final_df.to_csv('divorce_topic_modeling_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nğŸ’¾ ê²°ê³¼ê°€ 'divorce_topic_modeling_results.csv'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca0313-3fb6-4da5-ae4b-276f7fc247df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "final_results = combined_df[['input_processed', 'topic_label', 'topic_interpretation']].copy()\n",
    "final_results.to_csv('topic_modeling_results.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"í† í”½ ëª¨ë¸ë§ ì™„ë£Œ! ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"- ìµœì  í† í”½ ìˆ˜: {optimal_topics}\")\n",
    "print(f\"- í† í”½ ë¼ë²¨ ë¶„í¬:\")\n",
    "for topic_id, count in combined_df['topic_label'].value_counts().sort_index().items():\n",
    "    interpretation = topic_interpretation.get(topic_id, \"í•´ì„ í•„ìš”\")\n",
    "    print(f\"  í† í”½ {topic_id} ({interpretation}): {count}ê°œ ë¬¸ì„œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556bd0f-60da-424d-a90e-2fbabde98447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc5900-7d1d-4a4d-928a-b5f70e5abb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82935459-81da-4b92-abfc-48400300f93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f2b8e-0636-4070-beba-8caf02f25f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb85b3-032e-4853-bf63-92bf6a364135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6150ac-bcd4-47c0-9b39-1de61e4449eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['clean_text']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927d537-e9ce-4008-9bdd-efda36bfeca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# TF-IDF ë²¡í„°í™”\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f34e7-cfec-4fa7-86e2-7948820f895b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569b1df-15c7-45c7-b66f-517cf644b5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071737ee-fcde-4dde-bdd5-ceef2c244e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefa264-9806-476e-a9de-1daffe693baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
