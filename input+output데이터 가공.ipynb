{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7221c93b-8962-4591-b9ea-e87ecf3adcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17cd531-0ed3-4d9a-9d1a-9625025f7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv(\"C:/Users/82105/Downloads/combined.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3705e500-13cf-4022-a794-67a69e589bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리를 적용할 컬럼을 확인합니다. 'input'과 'output' 컬럼이 중요해 보입니다.\n",
    "# 혹시 데이터가 없는 경우(NaN)를 대비해 빈 문자열로 채워줍니다.\n",
    "combined_df['input'] = combined_df['input'].fillna('')\n",
    "combined_df['output'] = combined_df['output'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95628dc-5ed0-4aed-91a6-cff7e705b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 3 : 답변 검색 모델을 위한 '통합' 텍스트 컬럼 생성\n",
    "combined_df['text_combined'] = combined_df['input'].astype(str) + \" \" + combined_df['output'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c82c47-4b17-42c2-8167-e8a40f12be90",
   "metadata": {},
   "source": [
    "# 1단계 : KoNLPy를 사용한 정교한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa7e1ed-5e11-45b2-9981-82a28e6c8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 사용자 정의 불용어 리스트: 이 리스트에 있는 단어는 무조건 제거됩니다.\n",
    "#  추가로 의미 없게 자주 등장하는 단어가 있다면 리스트에 계속 추가해 나가시면 더욱 정교한 분석이 가능할 것\n",
    "# 분석하며 계속 추가해나갈 수 있습니다.\n",
    "KOREAN_STOPWORDS = [\n",
    "    # 일반적인 한글 불용어\n",
    "    '이', '그', '저', '것', '수', '등', '들', '더', '없', '있', '하', '되', '않', \n",
    "    '나', '우리', '너', '당신', '같', '또', '만', '수', '것', '때', '등', '년', '월', '일',\n",
    "    '때문', '정도', '사실', '생각', '경우', '문제', '방법', '상황', '내용', '결과',\n",
    "\n",
    "    # 법률 도메인 일반 불용어  \n",
    "    '가능하다', '가능', '가다', '되다', '하다', '있다', '없다', '않다', '된다', '한다',\n",
    "    '어떻게', '어떤', '무엇', '언제', '어디서', '왜', '누가', '얼마', '몇',\n",
    "    '알고', '싶다', '궁금하다', '문의', '질문', '답변', '설명', '이해',\n",
    "    '과정', '절차', '이후', '다음', '먼저', '그리고', '그러나', '하지만', '그래서', '제자',\n",
    "\n",
    "    # 법률 질문 패턴 불용어\n",
    "    '해야', '하면', '경우', '때는', '어느', '무슨', '어디', '누구' , ' 가지다' , '가지'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d423d8fa-bd1c-42d8-8173-bdf96d027c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protect_term(match):\n",
    "    # 찾은 문자열(예: '제3자')을 고유한 토큰(예: '###TOKEN0###')으로 매핑\n",
    "    token = f\"###TOKEN{len(protected_matches)}###\"\n",
    "    protected_matches[token] = match.group(0)\n",
    "    return token\n",
    "\n",
    "def preprocess_text(text):  # 전처리 함수 \n",
    "    \"\"\"\n",
    "    한국어 텍스트를 입력받아 전처리하는 함수:\n",
    "    1. '제3자' 형태의 단어를 임시 토큰으로 보호/복원하여 숫자를 보존.\n",
    "    2. 불필요한 한글 이외의 문자를 제거.\n",
    "    3. 형태소 분석 및 어간 추출.\n",
    "    4. 불용어 및 1글자 단어 제거.\n",
    "    \"\"\"\n",
    "    # 1단계: 한글, 공백을 제외한 모든 문자 제거\n",
    "    # re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)는 한글과 공백만 남기고 나머지는 지우라는 의미\n",
    "    # 제숫자 + 한글 글자 + 선택적 조사까지 포함\n",
    "    # 딕셔너리를 함수 내부에 선언하여 매 호출마다 초기화 (핵심 수정 사항)\n",
    "    protected_matches = {}\n",
    "\n",
    "    def protect_term(match):\n",
    "        # 찾은 문자열(예: '제3자')을 고유한 토큰(예: '###TOKEN0###')으로 매핑\n",
    "        token = f\"###TOKEN{len(protected_matches)}###\"\n",
    "        protected_matches[token] = match.group(0)\n",
    "        return match.group(0) # 일단 원본 단어 그대로 둔 채로 형태소 분석 진행\n",
    "\n",
    "\n",
    "    # 1단계: '제3자' 형태의 단어를 임시 토큰으로 치환하여 보호\n",
    "    re.sub(r'(제\\d+[가-힣]+)', protect_term, text)\n",
    "\n",
    "    # 2단계: 한글, 공백, 보호 토큰의 문자(#)만 남기고 제거\n",
    "    text = re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣# ]', '', text)\n",
    "\n",
    "    # 2단계: Okt 형태소 분석기를 이용한 토큰화 및 어간 추출(Stemming)\n",
    "    # okt.pos(text, stem=True)는 문장을 (단어, 품사) 형태로 나누고, '하다', '먹다'처럼 원형으로 만들어줍니다.\n",
    "    # 예: \"먹었었고\" -> ('먹다', 'Verb')\n",
    "    # Okt 형태소 분석기 객체 생성\n",
    "    okt = Okt()\n",
    "    word_tokens = okt.pos(text, stem=True)\n",
    "\n",
    "    # 3단계: 불용어 제거\n",
    "    # 형태소 분석 후 의미 있는 단어만 추출하는 필터링 과정\n",
    "    # 의미를 가지는 명사, 동사, 형용사, 부사 중에서 1글자 이상인 단어만 추출합니다.\n",
    "    # 품사 태그가 'Josa'(조사), 'Eomi'(어미), 'Punctuation'(구두점) 등인 단어들을 제거합니다.\n",
    "    meaningful_words = []\n",
    "    for word, pos in word_tokens:\n",
    "        # 1글자 단어 필터링은 의미있는 품사에서만 수행\n",
    "        is_meaningful_pos = pos in ['Noun', 'Verb', 'Adjective', 'Adverb']\n",
    "        \n",
    "        if is_meaningful_pos:\n",
    "            # 형태소 분석된 결과 그대로 '제3자'는 Noun으로 분류될 가능성이 높으며, 길이가 3입니다.\n",
    "            if len(word) > 1 and word not in KOREAN_STOPWORDS:\n",
    "                meaningful_words.append(word)\n",
    "        # '제3자'와 같이 숫자+한글이 붙은 단어는 1글자가 아니므로 포함됩니다.\n",
    "   \n",
    "            \n",
    "     # 6. 보호된 단어 강제 포함 및 복원 (핵심)\n",
    "    # 보호 목록에 있던 단어들을 강제로 최종 리스트에 추가합니다.\n",
    "    # (이미 리스트에 분해되어 들어갔을 수 있지만, 완벽한 복원을 위해 강제 추가)\n",
    "    for original_term in protected_matches.values():\n",
    "        # '제3자' 자체를 하나의 단어로 명시적으로 추가\n",
    "        meaningful_words.append(original_term)\n",
    "        \n",
    "    # 7. 중복 제거 및 최종 문자열 반환\n",
    "    # Set을 이용해 중복 제거 후 리스트로 변환\n",
    "    final_words = list(set(meaningful_words))\n",
    "    # 최종적으로 공백으로 연결된 문자열을 반환합니다. \n",
    "    # 모델에 따라 리스트 형태(meaningful_words)를 그대로 사용할 수도 있습니다.\n",
    "    # 텍스트에서 분석에 의미 있는 핵심 단어들만 남긴 리스트 생성\n",
    "    return ' '.join(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3dd7f2-da96-4bd9-a26d-f5b3f742a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [목적 2] 답변 검색 모델용: 'text_combined' 컬럼을 전처리\n",
    "combined_df['combined_processed'] = combined_df['text_combined'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbaebee-7d6a-44bf-815d-f39d0bddca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 전처리 전/후 비교 ===\n",
      "원본 [0]: 혼인 중 발생한 금전거래와 관련하여 차용금 반환 청구권이 인정되는 조건은 무엇인가요? 혼인 중 발생한 금전거래에 대한 차용금 반환 청구권이 인정되기 위해서는 차용금 지급의 사실을 입증할 객관적인 금융거래 자료가 필요합니다. 또한 차용금의 변제를 독촉하는 증거가 존재해야 하며, 재산분할 관련 약정에서 명확하게 차용금에 대한 정함이 있어야 합니다.\n",
      "결과 [0]: 명확하다 인정 반환 조건 지급 금융 정함 또한 자료 필요하다 독촉 금전 증거 변제 존재 재산 관련 분할 혼인 약정 차용 대한 객관 발생 입증 청구권 거래\n",
      "\n",
      "원본 [1]: 제3자가 부부의 일방과 부정행위를 할 경우 어떤 법적 책임을 지게 되나요? 제3자는 타인의 부부공동생활에 개입하여 그 파탄을 초래하거나 본질적인 부부공동생활을 방해하여서는 안 됩니다. 제3자가 부부의 일방과 부정행위를 함으로써 부부공동생활을 침해하고 배우자에게 정신적 고통을 주는 행위는 원칙적으로 불법행위에 해당합니다. 따라서 제3자는 불법행위책임으로 인해 위자료를 지급할 의무가 있습니다.\n",
      "결과 [1]: 방해 정신 초래 인하다 타인 불법행위 따라서 지급 해당 지게 부정행위 본질 책임 침해 행위 부부 원칙 생활 파탄 의무 제3자가 주다 제3자는 배우자 고통 위자료 일방 거나 서다 법적 어떻다 개입\n",
      "\n",
      "원본 [2]: 민법 제840조 제1호에서 규정한 재판상 이혼사유 중 부정한 행위의 의미는 무엇인가요? 민법 제840조 제1호에 따른 재판상 이혼사유인 부정한 행위는 간통에 이르지 아니하였더라도 부부의 정조의무에 충실하지 않은 일체의 부정행위를 포함하는 넓은 개념으로 해석됩니다. 대법원 판례에 따르면 일방 배우자가 배우자 있는 다른 사람과 부정행위를 저지른 경우, 이는 이혼 사유로 인정될 수 있습니다.\n",
      "결과 [2]: 제1호에서 해석 인정 일체 제840조 제조 이다 민법 넓다 저지르다 의미 충실하다 아니다 이르다 부정행위 이혼 따르다 행위 부부 의무 대법원 간통 사람과 개념 배우자 일방 제1호에 포함 판례 규정 정조 제호 다른 사유 재판 부정\n",
      "\n",
      "원본 [3]: 배우자가 있는 사람과 부정행위를 한 경우 위자료의 액수는 어떻게 정해지나요? 위자료의 액수는 부정행위의 내용, 정도 및 기간, 결혼기간 및 가족관계, 부정행위가 부부공동생활에 미친 영향을 참작하여 정해집니다. 예를 들어, 부정행위가 장기간 지속되었거나 부부 공동생활에 큰 영향을 미친 경우 위자료 액수가 증가할 수 있습니다. 한 사건에서는 이러한 기준에 따라 위자료 액수가 13,000,000원으로 정해진 사례가 있습니다.\n",
      "결과 [3]: 기준 사례 관계 부정행위 이러하다 따르다 사건 부부 액수 되어다 생활 크다 사람과 증가 배우자 위자료 장기간 가족 기간 정해진 지속 정해지다 결혼 들다 참작 영향 어떻다 미치다\n",
      "\n",
      "원본 [4]: 사해행위 취소 및 원상회복청구 소송에서 다른 채권자의 청구가 부적법해지는 상황은 어떤 경우인가요? 사해행위 취소 및 원상회복청구 소송에서 다른 채권자의 청구가 부적법해지는 경우는, 동일한 사해행위에 대해 먼저 진행된 다른 채권자의 취소 및 원상회복청구가 승소하여 판결이 확정되고 재산이나 가액의 회복이 이미 완료된 경우입니다. 예를 들어, 경기신용보증재단이 채무자 B와 C에 대해 사해행위로 인정된 재산분할계약을 취소하고 원상회복을 통해 배상금을 지급받은 상황에서, 동일한 사해행위에 대해 다른 채권자가 추가로 취소 및 원상회복청구를 하더라도 이로 인해 권리보호의 이익이 중복되므로 부적법하게 됩니다.\n",
      "결과 [4]: 인하다 인정 신용 지급 진행 완료 이다 통해 적법하다 가액 소송 동일하다 이로 추가 원상회복 오다 회복 확정 배상금 이미 채무자 중복 채권자 취소 받다 재산 보호 계약 대해 청구 부적 판결 분할 재단 들다 권리 승소 다른 법해 보증 이익 어떻다 사해행위\n",
      "\n",
      "=== 각 목적에 맞게 생성된 전처리 컬럼들 ===\n",
      "                                  combined_processed\n",
      "0  명확하다 인정 반환 조건 지급 금융 정함 또한 자료 필요하다 독촉 금전 증거 변제 ...\n",
      "1  방해 정신 초래 인하다 타인 불법행위 따라서 지급 해당 지게 부정행위 본질 책임 침...\n",
      "2  제1호에서 해석 인정 일체 제840조 제조 이다 민법 넓다 저지르다 의미 충실하다 ...\n",
      "3  기준 사례 관계 부정행위 이러하다 따르다 사건 부부 액수 되어다 생활 크다 사람과 ...\n",
      "4  인하다 인정 신용 지급 진행 완료 이다 통해 적법하다 가액 소송 동일하다 이로 추가...\n"
     ]
    }
   ],
   "source": [
    "# 결과 비교를 위해 원본과 처리된 결과를 나란히 출력\n",
    "print(\"\\n=== 전처리 전/후 비교 ===\")\n",
    "for i in range(5):\n",
    "    print(f\"원본 [{i}]: {combined_df['text_combined'].iloc[i]}\")\n",
    "    print(f\"결과 [{i}]: {combined_df['combined_processed'].iloc[i]}\\n\")\n",
    "\n",
    "\n",
    "# 전처리된 데이터가 포함된 데이터프레임 확인\n",
    "print(\"=== 각 목적에 맞게 생성된 전처리 컬럼들 ===\")\n",
    "print(combined_df[['combined_processed']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe061395-c92e-4bd9-be79-206455fe4ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 4447 행\n",
      "컬럼: ['doc_id', 'casetype', 'casenames', 'input', 'output', 'fileName', 'announce_date', 'decision_date', 'response_date', 'is_divorce', 'input_processed', 'topic_label', 'text_combined', 'combined_processed']\n",
      "topic_label 초기값: topic_label\n",
      "-1    4447\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "combined_df['topic_label'] = -1  # 초기값 -1로 설정\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"데이터 크기: {len(combined_df)} 행\")\n",
    "print(f\"컬럼: {list(combined_df.columns)}\")\n",
    "print(f\"topic_label 초기값: {combined_df['topic_label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4c412d-aaae-441a-830a-2c8c8fbbe31f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/82105/Downloads/combined.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "combined_df.to_csv(\"C:/Users/82105/Downloads/combined.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a562423-4084-4ad2-9f5f-69e009facd7a",
   "metadata": {},
   "source": [
    "# 2단계: 최적화된 TF-IDF 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9607351f-acfa-4bfe-a76a-17f89b60d18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 행렬 크기: (4447, 5000)\n",
      "문서 수: 4447\n",
      "특성(단어) 수: 5000\n",
      "상위 10개 특성: ['가능성' '가능성 결정' '가능성 위자료' '가능성 피해' '가동' '가사' '가사소송법' '가압류' '가압류 간주'\n",
      " '가압류 기각']\n",
      "상위 20개 특성: ['가능성' '가능성 결정' '가능성 위자료' '가능성 피해' '가동' '가사' '가사소송법' '가압류' '가압류 간주'\n",
      " '가압류 기각' '가압류 명시' '가압류 불법행위' '가압류 산정' '가압류 선고' '가압류 의거' '가압류 제조' '가압류 조건'\n",
      " '가압류 중단' '가압류 채권' '가압류 통상']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터를 머신러닝 알고리즘이 처리할 수 있는 수치 벡터로 변환\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# TF-IDF Vectorizer 설정\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # 상위 빈도 1000개 단어만 사용\n",
    "    min_df=2,              # 최소 2개 문서에 나타나야 포함\n",
    "    max_df=0.7,           # 80% 이상 문서에 나타나는 단어 제거\n",
    "    lowercase=False,         # 소문자 변환 x\n",
    "    ngram_range=(1, 2),     # 1-gram, 2-gram 모두 사용\n",
    "    sublinear_tf=True,            # TF 값에 로그 스케일 적용 (성능 향상)\n",
    "    token_pattern=r'[가-힣]{2,}', # 한글 2글자 이상\n",
    ")\n",
    "\n",
    "# TF-IDF 행렬 생성\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_df['combined_processed'])  # X는 (문서 수 × 단어 수) 크기의 희소 행렬\n",
    "\n",
    "print(f\"TF-IDF 행렬 크기: {tfidf_matrix.shape}\")\n",
    "print(f\"문서 수: {tfidf_matrix.shape[0]}\")\n",
    "print(f\"특성(단어) 수: {tfidf_matrix.shape[1]}\")\n",
    "\n",
    "# 특성 이름(단어들) 확인\n",
    "# vectorizer.get_feature_names_out()로 어떤 단어가 벡터에 들어갔는지 확인 가능\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"상위 10개 특성: {feature_names[:10]}\")\n",
    "print(f\"상위 20개 특성: {feature_names[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b5455-f5f5-4a05-9e3e-7a8653e7eaac",
   "metadata": {},
   "source": [
    "# 3단계: 토픽 개수 최적화\n",
    "# 최적의 토픽 개수를 찾기 위해 여러 지표를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a4c537-f20b-4dc7-85d6-c910e5a3bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽 수 2: Perplexity = 6325.4513\n",
      "토픽 수 3: Perplexity = 6982.2974\n",
      "토픽 수 4: Perplexity = 7436.3214\n",
      "토픽 수 5: Perplexity = 8321.9306\n",
      "토픽 수 6: Perplexity = 8012.8821\n",
      "토픽 수 7: Perplexity = 9517.6950\n",
      "최적의 토픽 수 (Perplexity 기준): 2\n"
     ]
    }
   ],
   "source": [
    "# 4-1. LDA Perplexity 계산 (낮을수록 좋음)\n",
    "\n",
    "# 토픽 개수별 Perplexity 계산\n",
    "perplexity_scores = []\n",
    "topic_range = range(2, 8)  # 2~10개 토픽 테스트\n",
    "\n",
    "for n_topics in topic_range:\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics, \n",
    "        random_state=42,\n",
    "        max_iter=100,\n",
    "        doc_topic_prior=0.1,      # 문서-토픽 분포 조정\n",
    "        topic_word_prior=0.01     # 토픽-단어 분포 조정\n",
    "    )\n",
    "    lda.fit(tfidf_matrix)\n",
    "    perplexity = lda.perplexity(tfidf_matrix)\n",
    "    perplexity_scores.append(perplexity)\n",
    "    print(f\"토픽 수 {n_topics}: Perplexity = {perplexity:.4f}\")\n",
    "\n",
    "# 최적의 토픽 수 선택\n",
    "optimal_topics = topic_range[np.argmin(perplexity_scores)]\n",
    "print(f\"최적의 토픽 수 (Perplexity 기준): {optimal_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242310eb-cbb6-427f-a518-0b4444c43c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1: WCSS=4292.7656\n",
      "K=2: WCSS=4237.5112\n",
      "K=3: WCSS=4189.1095\n",
      "K=4: WCSS=4168.0205\n",
      "K=5: WCSS=4149.2559\n",
      "K=6: WCSS=4119.1668\n",
      "K=7: WCSS=4105.0661\n"
     ]
    }
   ],
   "source": [
    "# 4-2. K-Means Elbow Method (WCSS)\n",
    "\n",
    "# K-Means의 WCSS 계산\n",
    "wcss_scores = []\n",
    "k_range = range(1, 8)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    wcss_scores.append(kmeans.inertia_)\n",
    "    print(f\"K={k}: WCSS={kmeans.inertia_:.4f}\")\n",
    "\n",
    "# Elbow Point 시각화로 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212dfc2e-e124-4af3-9c61-ba9c4570986c",
   "metadata": {},
   "source": [
    "# 더 정확한 토픽 모델링을 위해 Gensim 라이브러리를 사용\n",
    "# 고급 기법: Gensim LDA + Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfca808f-295b-4e43-8d76-656f2d0c364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽 수 2: Coherence = 0.5749\n",
      "토픽 수 3: Coherence = 0.5218\n",
      "토픽 수 4: Coherence = 0.5380\n",
      "토픽 수 5: Coherence = 0.4985\n",
      "토픽 수 6: Coherence = 0.5457\n",
      "토픽 수 7: Coherence = 0.5722\n",
      "토픽 수 8: Coherence = 0.5894\n",
      "토픽 수 9: Coherence = 0.5492\n",
      "토픽 수 10: Coherence = 0.5496\n",
      "최적 토픽 수 (Coherence 기준): 8\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# 텍스트를 토큰 리스트로 변환\n",
    "texts = [text.split() for text in combined_df['combined_processed']]\n",
    "\n",
    "# Dictionary 생성\n",
    "dictionary = Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "\n",
    "# Corpus 생성  \n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 최적 토픽 수 찾기 (Coherence Score 사용)\n",
    "coherence_scores = []\n",
    "topic_range = range(2, 11)\n",
    "\n",
    "for num_topics in topic_range:\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary, \n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=10\n",
    "    )\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        model=lda_model, \n",
    "        texts=texts, \n",
    "        dictionary=dictionary, \n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "    print(f\"토픽 수 {num_topics}: Coherence = {coherence_score:.4f}\")\n",
    "\n",
    "optimal_topics = topic_range[np.argmax(coherence_scores)]\n",
    "print(f\"최적 토픽 수 (Coherence 기준): {optimal_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a406333-6aba-45b9-8699-020df2b53bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891d7de-3b9f-439d-ac62-6a5aef581b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88e5f9-68b2-4071-8e24-bed459b30cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dad42-c121-470b-a699-4fa9f7ca9804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f7922-aa2a-4a10-ba05-fd7fa40854d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65197e-83f5-485d-9c2c-5fbcb62ed1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d45157-a19f-4cc6-9b62-02e8170063ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc7052-b212-4b2a-b44e-917357488284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab3727-3cf1-4865-be1e-a5539b3c4297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
